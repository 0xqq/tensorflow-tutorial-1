{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import glob\n",
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "from tensorflow.python.framework.ops import reset_default_graph\n",
    "\n",
    "def onehot(t, num_classes):\n",
    "    out = np.zeros((t.shape[0], num_classes))\n",
    "    for row, col in enumerate(t):\n",
    "        out[row, col] = 1\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle challenge\n",
    "\n",
    "In this lab we will work on a data science challenge from `kaggle.com`.\n",
    "Kaggle is a website to participate in real life challenges.\n",
    "Most competitions on kaggle have a dataset, an accuracy metric and a leaderboard to compare submissions.\n",
    "You can read more about kaggle [here](https://www.kaggle.com/about).\n",
    "\n",
    "OBS: You will need a kaggle account for this exercise!\n",
    "\n",
    "The challenge we will pursue is the [_Leaf Classification_](https://www.kaggle.com/c/leaf-classification) challenge.\n",
    "This is an image recognition challenge where each image is supplemented with three feature vectors (a shape contiguous descriptor, an interior texture histogram, and a ﬁne-scale margin histogram).\n",
    "\n",
    "The first task in a kaggle competition is to download, understand and preprocess the data, which we will do in the first section.\n",
    "\n",
    "Afterwards, we will look into the type of neural network best suited for handling this type of data (for images, usually convolutional neural networks does a pretty good job).\n",
    "\n",
    "Lastly, we will train the model and put the outputs in a submission file that we can submit to kaggle.\n",
    "Convolution neural networks are one of the most succesfull types of neural networks for image recognition and an integral part of reigniting the interest in neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download data\n",
    "\n",
    "Go to the [data section](https://www.kaggle.com/c/leaf-classification/data) of the Leaf Classification competition on kaggle.\n",
    "\n",
    "Next, download all of the available data (`sample_submission.csv`, `train.csv`, `test.csv`, `images`), accept the disclaimer if asked and unzip all folders into the `lab4` folder.\n",
    "Such that\n",
    "\n",
    "```\n",
    ">ls $PATH\\_TO\\_FOLDER/tensorflow_tutorial/lab4\n",
    "data.py  images  lab4_Kaggle.ipynb  README.md  sample_submission.csv  test.csv  train.csv  train.py\n",
    "```\n",
    "\n",
    "Below we will try to load the data into memory and print it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "image_paths = glob.glob(\"images/*\")\n",
    "print \"Amount of images =\", len(image_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# now plot 10 images\n",
    "# as we need all images to have the same dimensionality, we will resize and plot\n",
    "# make the images as small as possible, until the difference between starts to get blurry\n",
    "for i in range(10):\n",
    "    image = imread(image_paths[i], as_grey=True)\n",
    "    #image = resize(image, output_shape=(100, 100))\n",
    "    plt.imshow(image, cmap='gray')\n",
    "    plt.title(\"name: %s \\n shape:%s\" % (image_paths[i], image.shape))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# now loading the train.csv to find features for each training point\n",
    "train = pd.read_csv('train.csv')\n",
    "# notice how we \"only\" have 990 (989+0 elem) images for training, the rest is for testing\n",
    "train.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# now do similar as in train example above for test.csv\n",
    "test = pd.read_csv('test.csv')\n",
    "# notice that we do not have species here, we need to predict that ..!\n",
    "test.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# and now do similar as in train example above for test.csv\n",
    "sample_submission = pd.read_csv('sample_submission.csv')\n",
    "# accordingly to these IDs we need to provide the probability of a given plant being present\n",
    "sample_submission.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# name all columns in train, should be 3 different columns with 64 values each\n",
    "print train.columns[2::64]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# try and extract and plot columns\n",
    "X = train.as_matrix(columns=train.columns[2:])\n",
    "print \"X.shape,\", X.shape\n",
    "margin = X[:, :64]\n",
    "shape = X[:, 64:128]\n",
    "texture = X[:, 128:]\n",
    "print \"margin.shape,\", margin.shape\n",
    "print \"shape.shape,\", shape.shape\n",
    "print \"texture.shape,\", texture.shape\n",
    "# let us plot some of the features\n",
    "plt.figure(figsize=(21,7))\n",
    "for i in range(3):\n",
    "    plt.subplot(3,3,1+i*3)\n",
    "    plt.plot(margin[i])\n",
    "    if i == 0:\n",
    "        plt.title('Margin', fontsize=20)\n",
    "    plt.axis('off')\n",
    "    plt.subplot(3,3,2+i*3)\n",
    "    plt.plot(shape[i])\n",
    "    if i == 0:\n",
    "        plt.title('Shape', fontsize=20)\n",
    "    plt.axis('off')\n",
    "    plt.subplot(3,3,3+i*3)\n",
    "    plt.plot(texture[i])\n",
    "    if i == 0:\n",
    "        plt.title('Texture', fontsize=20)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise\n",
    "\n",
    "1. Test various resizings of the image until you have found the smallest resizing of the image where you can still see differentiate between the images.\n",
    "\n",
    "2. From the illustration of the Margin, Shape and Texture, what do you see? And how can it be used to classify?\n",
    "\n",
    "3. Describe what network you would build and how you would represent the data points (image, margin, shape and texture)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Building data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import glob\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "from tensorflow.python.framework.ops import reset_default_graph\n",
    "import os\n",
    "import itertools\n",
    "\n",
    "def onehot(t, num_classes):\n",
    "    out = np.zeros((t.shape[0], num_classes))\n",
    "    for row, col in enumerate(t):\n",
    "        out[row, col] = 1\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class load_data():\n",
    "    # data_train, data_test and le are public\n",
    "    def __init__(self, train_path, test_path, image_paths, image_shape=(128, 128)):\n",
    "        train_df = pd.read_csv(train_path)\n",
    "        test_df = pd.read_csv(test_path)\n",
    "        image_paths = image_paths\n",
    "        image_shape = image_shape\n",
    "        self._load(train_df, test_df, image_paths, image_shape)\n",
    "        \n",
    "    def _load(self, train_df, test_df, image_paths, image_shape):\n",
    "        print \"loading data ...\"\n",
    "        # load train.csv\n",
    "        path_dict = self._path_to_dict(image_paths) # numerate image paths and make it a dict\n",
    "        # merge image paths with data frame\n",
    "        train_image_df = self._merge_image_df(train_df, path_dict)\n",
    "        test_image_df = self._merge_image_df(test_df, path_dict)\n",
    "        # label encoder-decoder (self. because we need it later)\n",
    "        self.le = LabelEncoder().fit(train_image_df['species'])\n",
    "        # labels for train\n",
    "        t_train = self.le.transform(train_image_df['species'])\n",
    "        # getting data\n",
    "        train_data = self._make_dataset(train_image_df, image_shape, t_train)\n",
    "        test_data = self._make_dataset(test_image_df, image_shape)        \n",
    "        # need to reformat the train for validation split reasons in the batch_generator\n",
    "        self.train = self._format_dataset(train_data, for_train=True)\n",
    "        self.test = self._format_dataset(test_data, for_train=False)\n",
    "        print \"data loaded\"\n",
    "        \n",
    "\n",
    "    def _path_to_dict(self, image_paths):\n",
    "        path_dict = dict()\n",
    "        for image_path in image_paths:\n",
    "            num_path = int(os.path.basename(image_path[:-4]))\n",
    "            path_dict[num_path] = image_path\n",
    "        return path_dict\n",
    "\n",
    "    def _merge_image_df(self, df, path_dict):\n",
    "        split_path_dict = dict()\n",
    "        for index, row in df.iterrows():\n",
    "            split_path_dict[row['id']] = path_dict[row['id']]\n",
    "        image_frame = pd.DataFrame(split_path_dict.values(), columns=['image'])\n",
    "        df_image =  pd.concat([image_frame, df], axis=1)\n",
    "        return df_image\n",
    "    \n",
    "\n",
    "    def _make_dataset(self, df, image_shape, t_train=None):\n",
    "        if t_train is not None:\n",
    "            print \"loading train ...\"\n",
    "        else:\n",
    "            print \"loading test ...\"\n",
    "        # make dataset\n",
    "        data = dict()\n",
    "        # merge image with 3x64 features\n",
    "        for i, dat in enumerate(df.iterrows()):\n",
    "            index, row = dat\n",
    "            sample = dict()\n",
    "            if t_train is not None:\n",
    "                features = row.drop(['id', 'species', 'image'], axis=0).values\n",
    "            else:\n",
    "                features = row.drop(['id', 'image'], axis=0).values\n",
    "            sample['margin'] = features[:64]\n",
    "            sample['shape'] = features[64:128]\n",
    "            sample['texture'] = features[128:]\n",
    "            if t_train is not None:\n",
    "                sample['t'] = np.asarray(t_train[i], dtype='int32')\n",
    "            image = imread(row['image'], as_grey=True)\n",
    "            image = resize(image, output_shape=image_shape)\n",
    "            image = np.expand_dims(image, axis=2)\n",
    "            sample['image'] = image   \n",
    "            data[row['id']] = sample\n",
    "            if i % 100 == 0:\n",
    "                print \"\\t%d of %d\" % (i, len(df))\n",
    "        return data\n",
    "\n",
    "    def _format_dataset(self, df, for_train):\n",
    "        # making arrays with all data in, is nessesary when doing validation split\n",
    "        data = dict()\n",
    "        value = df.values()[0]\n",
    "        img_tot_shp = tuple([len(df)] + list(value['image'].shape))\n",
    "        data['images'] = np.zeros(img_tot_shp, dtype='float32')\n",
    "        feature_tot_shp = (len(df), 64)\n",
    "        data['margins'] = np.zeros(feature_tot_shp, dtype='float32')\n",
    "        data['shapes'] = np.zeros(feature_tot_shp, dtype='float32')\n",
    "        data['textures'] = np.zeros(feature_tot_shp, dtype='float32')\n",
    "        if for_train:\n",
    "            data['ts'] = np.zeros((len(df),), dtype='int32')\n",
    "        else:\n",
    "            data['ids'] = np.zeros((len(df),), dtype='int32')\n",
    "        for i, pair in enumerate(df.items()):\n",
    "            key, value = pair\n",
    "            data['images'][i] = value['image']\n",
    "            data['margins'][i] = value['margin']\n",
    "            data['shapes'][i] = value['shape']\n",
    "            data['textures'][i] = value['texture']\n",
    "            if for_train:\n",
    "                data['ts'][i] = value['t']\n",
    "            else:\n",
    "                data['ids'][i] = key\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data ...\n",
      "loading train ...\n",
      "\t0 of 990\n",
      "\t100 of 990\n",
      "\t200 of 990\n",
      "\t300 of 990\n",
      "\t400 of 990\n",
      "\t500 of 990\n",
      "\t600 of 990\n",
      "\t700 of 990\n",
      "\t800 of 990\n",
      "\t900 of 990\n",
      "loading test ...\n",
      "\t0 of 594\n",
      "\t100 of 594\n",
      "\t200 of 594\n",
      "\t300 of 594\n",
      "\t400 of 594\n",
      "\t500 of 594\n",
      "data loaded\n",
      "\n",
      "@@@Shape checking of data sets@@@\n",
      "\n",
      "TRAIN\n",
      "\timages\t(990, 128, 128, 1)0.462177\n",
      "\tmargins\t(990, 64)\t0.015625\n",
      "\tshapes\t(990, 64)\t0.000607\n",
      "\ttextures(990, 64)\t0.015625\n",
      "\tts\t 990\n",
      "\twhile training, batch_generator will onehot encode ts to (batch_size, num_classes)\n",
      "\n",
      "TEST\n",
      "\timages\t(594, 128, 128, 1)\t0.463148\n",
      "\tmargins\t(594, 64)\t0.015625\n",
      "\tshapes\t(594, 64)\t0.000604\n",
      "\ttextures(594, 64)\t0.015625\n",
      "\tids\t594\n"
     ]
    }
   ],
   "source": [
    "# loading data and setting up constants\n",
    "TRAIN_PATH = \"train.csv\"\n",
    "TEST_PATH = \"test.csv\"\n",
    "IMAGE_PATHS = glob.glob(\"images/*.jpg\")\n",
    "NUM_CLASSES = 99\n",
    "IMAGE_SHAPE = (128, 128, 1)\n",
    "NUM_FEATURES = 64 # for all three features, margin, shape and texture\n",
    "# train holds both X (input) and t (target/truth)\n",
    "data = load_data(train_path=TRAIN_PATH, test_path=TEST_PATH,\n",
    "                 image_paths=IMAGE_PATHS, image_shape=IMAGE_SHAPE[:2])\n",
    "# to visualize the size of the dimensions of the data\n",
    "print\n",
    "print \"@@@Shape checking of data sets@@@\"\n",
    "print\n",
    "print \"TRAIN\"\n",
    "print \"\\timages\\t%s%f\" % (data.train['images'].shape, data.train['images'].mean())\n",
    "print \"\\tmargins\\t%s\\t%f\" % (data.train['margins'].shape, data.train['margins'].mean())\n",
    "print \"\\tshapes\\t%s\\t%f\" % (data.train['shapes'].shape, data.train['shapes'].mean())\n",
    "print \"\\ttextures%s\\t%f\" % (data.train['textures'].shape, data.train['textures'].mean())\n",
    "print \"\\tts\\t %s\" % (data.train['ts'].shape)\n",
    "print \"\\twhile training, batch_generator will onehot encode ts to (batch_size, num_classes)\"\n",
    "print\n",
    "print \"TEST\"\n",
    "print \"\\timages\\t%s\\t%f\" % (data.test['images'].shape, data.test['images'].mean()) \n",
    "print \"\\tmargins\\t%s\\t%f\" % (data.test['margins'].shape, data.test['margins'].mean())\n",
    "print \"\\tshapes\\t%s\\t%f\" % (data.test['shapes'].shape, data.test['shapes'].mean())\n",
    "print \"\\ttextures%s\\t%f\" % (data.test['textures'].shape, data.test['textures'].mean())\n",
    "print \"\\tids\\t%s\" % (data.test['ids'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# batch generator\n",
    "\n",
    "While training, we will not directly access the entire dataset, instead we have a `batch_generator` function to give us inputs aligned with their targets/ids in a size that our model can handle in memory (batch\\_size).\n",
    "\n",
    "Furthermore, the `batch_generator` also handles validation splitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class batch_generator():\n",
    "    def __init__(self, data, batch_size=64, num_classes=99,\n",
    "                 num_iterations=5e3, num_features=64, seed=42, val_size=0.1):\n",
    "        print \"initiating batch generator\"\n",
    "        self._train = data.train\n",
    "        self._test = data.test\n",
    "        # get image size\n",
    "        value = self._train['images'][0]\n",
    "        self._image_shape = list(value.shape)\n",
    "        self._batch_size = batch_size\n",
    "        self._num_classes = num_classes\n",
    "        self._num_iterations = num_iterations\n",
    "        self._num_features = num_features\n",
    "        self._seed = seed\n",
    "        self._val_size = 0.1\n",
    "        self._valid_split()\n",
    "        print \"batch generator initiated ...\"\n",
    "\n",
    "    def _valid_split(self):\n",
    "        self._idcs_train, self._idcs_valid = iter(\n",
    "            StratifiedShuffleSplit(self._train['ts'],\n",
    "                                   n_iter=1,\n",
    "                                   test_size=self._val_size,\n",
    "                                   random_state=self._seed)).next()\n",
    "    def _shuffle_train(self):\n",
    "        np.random.shuffle(self._idcs_train)\n",
    "\n",
    "    def _batch_init(self, purpose):\n",
    "        assert purpose in ['train', 'valid', 'test']\n",
    "        batch_holder = dict()\n",
    "        batch_holder['margins'] = np.zeros((self._batch_size, self._num_features), dtype='float32')\n",
    "        batch_holder['shapes'] = np.zeros((self._batch_size, self._num_features), dtype='float32')\n",
    "        batch_holder['textures'] = np.zeros((self._batch_size, self._num_features), dtype='float32')\n",
    "        batch_holder['images'] = np.zeros(tuple([self._batch_size] + self._image_shape), dtype='float32')\n",
    "        if (purpose == \"train\") or (purpose == \"valid\"):\n",
    "            batch_holder['ts'] = np.zeros((self._batch_size, self._num_classes), dtype='float32')          \n",
    "        else:\n",
    "            batch_holder['ids'] = []\n",
    "        return batch_holder\n",
    "\n",
    "    def gen_valid(self):\n",
    "        batch = self._batch_init(purpose='train')\n",
    "        i = 0\n",
    "        for idx in self._idcs_valid:\n",
    "            batch['margins'][i] = self._train['margins'][idx]\n",
    "            batch['shapes'][i] = self._train['shapes'][idx]\n",
    "            batch['textures'][i] = self._train['textures'][idx]\n",
    "            batch['images'][i] = self._train['images'][idx]\n",
    "            batch['ts'][i] = onehot(np.asarray([self._train['ts'][idx]], dtype='float32'), self._num_classes)\n",
    "            i += 1\n",
    "            if i >= self._batch_size:\n",
    "                yield batch, i\n",
    "                batch = self._batch_init(purpose='valid')\n",
    "                i = 0\n",
    "        if i != 0:\n",
    "            yield batch, i\n",
    "\n",
    "    def gen_test(self):\n",
    "        batch = self._batch_init(purpose='test')\n",
    "        i = 0\n",
    "        for idx in range(len(self._test['ids'])):\n",
    "            batch['margins'][i] = self._test['margins'][idx]\n",
    "            batch['shapes'][i] = self._test['shapes'][idx]\n",
    "            batch['textures'][i] = self._test['textures'][idx]\n",
    "            batch['images'][i] = self._test['images'][idx]\n",
    "            batch['ids'].append(self._test['ids'][idx])\n",
    "            i += 1\n",
    "            if i >= self._batch_size:\n",
    "                yield batch, i\n",
    "                batch = self._batch_init(purpose='test')\n",
    "                i = 0\n",
    "        if i != 0:\n",
    "            yield batch, i\n",
    "            \n",
    "\n",
    "    def gen_train(self):\n",
    "        batch = self._batch_init(purpose='train')\n",
    "        iteration = 0\n",
    "        i = 0\n",
    "        while True:\n",
    "            # shuffling all batches\n",
    "            self._shuffle_train()\n",
    "            for idx in self._idcs_train:\n",
    "                # extract data from dict\n",
    "                batch['margins'][i] = self._train['margins'][idx]\n",
    "                batch['shapes'][i] = self._train['shapes'][idx]\n",
    "                batch['textures'][i] = self._train['textures'][idx]\n",
    "                batch['images'][i] = self._train['images'][idx]\n",
    "                batch['ts'][i] = onehot(np.asarray([self._train['ts'][idx]], dtype='float32'), self._num_classes)\n",
    "                i += 1\n",
    "                if i >= self._batch_size:\n",
    "                    yield batch\n",
    "                    batch = self._batch_init(purpose='train')\n",
    "                    i = 0\n",
    "                    iteration += 1\n",
    "                    if iteration >= self._num_iterations:\n",
    "                        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initiating batch generator\n",
      "batch generator initiated ...\n",
      "\n",
      "@@@Shape/mean checking of batches@@@\n",
      "\n",
      "TRAIN\n",
      "\timages, (64, 128, 128, 1)\n",
      "\tmargins, (64, 64)\n",
      "\tshapes, (64, 64)\n",
      "\ttextures, (64, 64)\n",
      "\tts, (64, 99)\n",
      "\n",
      "VALID\n",
      "\timages, (64, 128, 128, 1)\n",
      "\tmargins, (64, 64)\n",
      "\tshapes, (64, 64)\n",
      "\ttextures, (64, 64)\n",
      "\tts, (64, 99)\n",
      "\n",
      "TEST\n",
      "\timages, (64, 128, 128, 1)\n",
      "\tmargins, (64, 64)\n",
      "\tshapes, (64, 64)\n",
      "\ttextures, (64, 64)\n",
      "\tids, 64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:19: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    }
   ],
   "source": [
    "dummy_batch_gen = batch_generator(data, batch_size=64, num_classes=99, num_iterations=5e3, seed=42)\n",
    "train_batch = dummy_batch_gen.gen_train().next()\n",
    "valid_batch, i = dummy_batch_gen.gen_valid().next()\n",
    "test_batch, i = dummy_batch_gen.gen_test().next()\n",
    "\n",
    "print\n",
    "print \"@@@Shape/mean checking of batches@@@\"\n",
    "print\n",
    "print \"TRAIN\"\n",
    "print \"\\timages,\", train_batch['images'].shape\n",
    "print \"\\tmargins,\", train_batch['margins'].shape\n",
    "print \"\\tshapes,\", train_batch['shapes'].shape\n",
    "print \"\\ttextures,\", train_batch['textures'].shape\n",
    "print \"\\tts,\", train_batch['ts'].shape\n",
    "print\n",
    "print \"VALID\"\n",
    "print \"\\timages,\", valid_batch['images'].shape\n",
    "print \"\\tmargins,\", valid_batch['margins'].shape\n",
    "print \"\\tshapes,\", valid_batch['shapes'].shape\n",
    "print \"\\ttextures,\", valid_batch['textures'].shape\n",
    "print \"\\tts,\", valid_batch['ts'].shape\n",
    "print\n",
    "print \"TEST\"\n",
    "print \"\\timages,\", test_batch['images'].shape\n",
    "print \"\\tmargins,\", test_batch['margins'].shape\n",
    "print \"\\tshapes,\", test_batch['shapes'].shape\n",
    "print \"\\ttextures,\", test_batch['textures'].shape\n",
    "print \"\\tids,\", len(test_batch['ids'])\n",
    "# notice that mean is very different, which is why we use batch_norm in all input data in model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Documentation on contrib layers\n",
    "Check out the [github page](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/layers/python/layers/layers.py) for information on contrib layers (not well documented in their api)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.contrib.layers import fully_connected, convolution2d, flatten, batch_norm, max_pool2d, dropout\n",
    "from tensorflow.python.ops.nn import relu, elu, relu6, sigmoid, tanh, softmax\n",
    "from tensorflow.python.ops.nn import dynamic_rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv_pre(l_in, num_outputs, kernel_size, scope, stride=1):\n",
    "    return convolution2d(l_relu, num_outputs=num_outputs, kernel_size=kernel_size,\n",
    "                         stride=stride, normalize_fn=batch_norm, scope=scope)\n",
    "\n",
    "# pre-activation: http://arxiv.org/abs/1603.05027\n",
    "# wrapping convolutions and batch_norm\n",
    "def conv_pre(l_in, num_outputs, kernel_size, scope, stride=1):\n",
    "    l_norm = batch_norm(l_in)\n",
    "    l_relu = relu(l_norm)\n",
    "    return convolution2d(l_relu, num_outputs=num_outputs, kernel_size=kernel_size,\n",
    "                         stride=stride, activation_fn=None, scope=scope)\n",
    "# easy to use pool function\n",
    "def pool(l_in, scope, kernel_size=(3, 3)):\n",
    "    return max_pool2d(l_in, kernel_size=kernel_size, scope=scope) # (3, 3) has shown to work better than (2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# hyperameters of the model\n",
    "height, width, channels = IMAGE_SHAPE\n",
    "# resetting the graph ...\n",
    "reset_default_graph()\n",
    "\n",
    "# Setting up placeholder, this is where your data enters the graph!\n",
    "x_image_pl = tf.placeholder(tf.float32, [None, height, width, channels], name=\"x_image_pl\")\n",
    "x_margin_pl = tf.placeholder(tf.float32, [None, NUM_FEATURES], name=\"x_margin_pl\")\n",
    "x_shape_pl = tf.placeholder(tf.float32, [None, NUM_FEATURES], name=\"x_shape_pl\")\n",
    "x_texture_pl = tf.placeholder(tf.float32, [None, NUM_FEATURES], name=\"x_texture_pl\")\n",
    "is_training_pl = tf.placeholder(tf.bool, name=\"is_training_pl\")\n",
    "\n",
    "# Building the layers of the neural network\n",
    "# we define the variable scope, so we more easily can recognise our variables later\n",
    "\n",
    "## IMAGE\n",
    "#l_conv1_a = conv(x_image_pl, 16, (5, 5), scope=\"l_conv1_a\")\n",
    "#l_pool1 = pool(l_conv1_a, scope=\"l_pool1\")\n",
    "#l_conv2_a = conv(l_pool1, 16, (5, 5), scope=\"l_conv2_a\")\n",
    "#l_pool2 = pool(l_conv2_a, scope=\"l_pool2\")\n",
    "#l_conv3_a = conv(l_pool2, 16, (5, 5), scope=\"l_conv3_a\")\n",
    "#l_pool3 = pool(l_conv3_a, scope=\"l_pool3\")\n",
    "#l_conv4_a = conv(l_pool3, 16, (5, 5), scope=\"l_conv4_a\")\n",
    "#l_pool4 = pool(l_conv3_a, scope=\"l_pool4\")\n",
    "#l_flatten = flatten(l_pool4, scope=\"flatten\")\n",
    "\n",
    "## FEATURES\n",
    "# RNNs\n",
    "#margin_cell = tf.nn.rnn_cell.GRUCell(100)\n",
    "#shape_cell = tf.nn.rnn_cell.GRUCell(100)\n",
    "#texture_cell = tf.nn.rnn_cell.GRUCell(100)\n",
    "#_, margin_state = tf.nn.dynamic_rnn(cell=margin_cell,\n",
    "#    inputs=tf.expand_dims(batch_norm(x_margin_pl), 2), dtype=tf.float32, scope=\"margin_rnn\")\n",
    "#_, shape_state = tf.nn.dynamic_rnn(cell=shape_cell,\n",
    "#    inputs=tf.expand_dims(batch_norm(x_shape_pl), 2), dtype=tf.float32, scope=\"shape_rnn\")\n",
    "#_, texture_state = tf.nn.dynamic_rnn(cell=texture_cell,\n",
    "#    inputs=tf.expand_dims(batch_norm(x_texture_pl), 2), dtype=tf.float32, scope=\"texture_rnn\")\n",
    "\n",
    "## COMBINE\n",
    "features = tf.concat(concat_dim=1, values=[x_margin_pl, x_shape_pl, x_texture_pl], name=\"features\")\n",
    "#features = l_flatten\n",
    "#features = tf.concat(concat_dim=1, values=[margin_state, shape_state, texture_state], name=\"features\")\n",
    "features = batch_norm(features, scope='features_bn')\n",
    "#l2 = fully_connected(features, num_outputs=256, activation_fn=relu,\n",
    "#                     normalizer_fn=batch_norm, scope=\"l2\")\n",
    "#l2 = dropout(l2, is_training=is_training_pl, scope=\"l2_dropout\")\n",
    "y = fully_connected(features, NUM_CLASSES, activation_fn=softmax, scope=\"y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_image_pl, <bound method Tensor.get_shape of <tf.Tensor 'x_image_pl:0' shape=(?, 128, 128, 1) dtype=float32>>\n",
      "x_margin_pl, <bound method Tensor.get_shape of <tf.Tensor 'x_margin_pl:0' shape=(?, 64) dtype=float32>>\n"
     ]
    }
   ],
   "source": [
    "# PRINT NETWORK\n",
    "\n",
    "print \"x_image_pl,\", x_image_pl.get_shape\n",
    "print \"x_margin_pl,\", x_margin_pl.get_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# y_ is a placeholder variable taking on the value of the target batch.\n",
    "ts_pl = tf.placeholder(tf.float32, [None, NUM_CLASSES], name=\"targets_pl\")\n",
    "lr_pl = tf.placeholder(tf.float32, [], name=\"learning_rate_pl\")\n",
    "\n",
    "def loss_and_acc(preds):\n",
    "    # computing cross entropy per sample\n",
    "    cross_entropy = -tf.reduce_sum(ts_pl * tf.log(preds+1e-10), reduction_indices=[1])\n",
    "    # averaging over samples\n",
    "    loss = tf.reduce_mean(cross_entropy)\n",
    "    # if you want regularization\n",
    "    #reg_scale = 0.0001\n",
    "    #regularize = tf.contrib.layers.l2_regularizer(reg_scale)\n",
    "    #params = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)\n",
    "    #reg_term = sum([regularize(param) for param in params])\n",
    "    #loss += reg_term\n",
    "    # calculate accuracy\n",
    "    argmax_y = tf.to_int32(tf.argmax(preds, dimension=1))\n",
    "    argmax_t = tf.to_int32(tf.argmax(ts_pl, dimension=1))\n",
    "    correct = tf.to_float(tf.equal(argmax_y, argmax_t))\n",
    "    accuracy = tf.reduce_mean(correct)\n",
    "    return loss, accuracy, argmax_y\n",
    "\n",
    "# loss, accuracy and prediction\n",
    "loss, accuracy, prediction = loss_and_acc(y)\n",
    "\n",
    "# defining our optimizer\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n",
    "\n",
    "# applying the gradients\n",
    "train_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y (45, 99)\n"
     ]
    }
   ],
   "source": [
    "#Test the forward pass\n",
    "_img_shape = tuple([45]+list(IMAGE_SHAPE))\n",
    "_feature_shape = (45, NUM_FEATURES)\n",
    "_x_image = np.random.normal(0, 1, _img_shape).astype('float32') #dummy data\n",
    "_x_margin = np.random.normal(0, 1, _feature_shape).astype('float32')\n",
    "_x_shape = np.random.normal(0, 1, _feature_shape).astype('float32')\n",
    "_x_texture = np.random.normal(0, 1, _feature_shape).astype('float32')\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.initialize_all_variables())\n",
    "feed_dict = {x_image_pl: _x_image,\n",
    "             x_margin_pl: _x_margin,\n",
    "             x_shape_pl: _x_shape,\n",
    "             x_texture_pl: _x_texture,\n",
    "             is_training_pl: False}\n",
    "res_forward_pass = sess.run(fetches=[y], feed_dict=feed_dict)\n",
    "print \"y\", res_forward_pass[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initiating batch generator\n",
      "batch generator initiated ...\n",
      "\ttrain_loss \ttrain_acc \tvalid_loss \tvalid_acc\n",
      "0:\t  4.60\t\t  4.7\t\t  4.02\t\t  1.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:19: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100:\t  3.71\t\t  30.9\t\t  2.42\t\t  49.2\n",
      "200:\t  2.23\t\t  82.6\t\t  1.50\t\t  73.6\n",
      "300:\t  1.39\t\t  95.0\t\t  1.00\t\t  80.8\n",
      "400:\t  0.91\t\t  97.7\t\t  0.71\t\t  81.9\n",
      "500:\t  0.64\t\t  99.1\t\t  0.54\t\t  82.4\n",
      "600:\t  0.47\t\t  99.4\t\t  0.44\t\t  82.4\n",
      "700:\t  0.37\t\t  99.6\t\t  0.36\t\t  82.4\n",
      "800:\t  0.29\t\t  99.7\t\t  0.31\t\t  83.4\n",
      "900:\t  0.24\t\t  99.8\t\t  0.27\t\t  83.4\n",
      "1000:\t  0.20\t\t  99.8\t\t  0.24\t\t  83.4\n",
      "1100:\t  0.17\t\t  99.9\t\t  0.21\t\t  84.0\n",
      "1200:\t  0.15\t\t  99.9\t\t  0.19\t\t  84.0\n",
      "1300:\t  0.13\t\t  99.9\t\t  0.18\t\t  84.0\n",
      "1400:\t  0.11\t\t  99.9\t\t  0.16\t\t  84.0\n",
      "1500:\t  0.10\t\t  100.0\t\t  0.15\t\t  84.0\n",
      "1600:\t  0.09\t\t  100.0\t\t  0.14\t\t  84.0\n",
      "1700:\t  0.08\t\t  100.0\t\t  0.13\t\t  84.0\n",
      "1800:\t  0.07\t\t  100.0\t\t  0.12\t\t  84.0\n",
      "1900:\t  0.07\t\t  100.0\t\t  0.12\t\t  84.0\n",
      "2000:\t  0.06\t\t  100.0\t\t  0.11\t\t  84.0\n",
      "2100:\t  0.05\t\t  100.0\t\t  0.10\t\t  84.0\n",
      "2200:\t  0.05\t\t  100.0\t\t  0.10\t\t  84.0\n",
      "2300:\t  0.05\t\t  100.0\t\t  0.09\t\t  84.0\n",
      "2400:\t  0.04\t\t  100.0\t\t  0.09\t\t  84.0\n",
      "2500:\t  0.04\t\t  100.0\t\t  0.08\t\t  84.0\n",
      "2600:\t  0.04\t\t  100.0\t\t  0.08\t\t  84.0\n",
      "2700:\t  0.03\t\t  100.0\t\t  0.08\t\t  84.0\n",
      "2800:\t  0.03\t\t  100.0\t\t  0.07\t\t  84.0\n",
      "2900:\t  0.03\t\t  100.0\t\t  0.07\t\t  84.0\n",
      "3000:\t  0.03\t\t  100.0\t\t  0.07\t\t  84.0\n",
      "3100:\t  0.03\t\t  100.0\t\t  0.07\t\t  84.0\n",
      "3200:\t  0.02\t\t  100.0\t\t  0.06\t\t  84.0\n",
      "3300:\t  0.02\t\t  100.0\t\t  0.06\t\t  84.0\n",
      "3400:\t  0.02\t\t  100.0\t\t  0.06\t\t  84.0\n",
      "3500:\t  0.02\t\t  100.0\t\t  0.06\t\t  84.0\n",
      "3600:\t  0.02\t\t  100.0\t\t  0.06\t\t  84.0\n",
      "3700:\t  0.02\t\t  100.0\t\t  0.05\t\t  84.0\n",
      "3800:\t  0.02\t\t  100.0\t\t  0.05\t\t  84.0\n",
      "3900:\t  0.02\t\t  100.0\t\t  0.05\t\t  84.0\n",
      "4000:\t  0.01\t\t  100.0\t\t  0.05\t\t  84.0\n",
      "4100:\t  0.01\t\t  100.0\t\t  0.05\t\t  84.0\n",
      "4200:\t  0.01\t\t  100.0\t\t  0.05\t\t  84.0\n",
      "4300:\t  0.01\t\t  100.0\t\t  0.05\t\t  84.0\n",
      "4400:\t  0.01\t\t  100.0\t\t  0.04\t\t  84.0\n",
      "4500:\t  0.01\t\t  100.0\t\t  0.04\t\t  84.0\n",
      "4600:\t  0.01\t\t  100.0\t\t  0.04\t\t  84.0\n",
      "4700:\t  0.01\t\t  100.0\t\t  0.04\t\t  84.0\n",
      "4800:\t  0.01\t\t  100.0\t\t  0.04\t\t  84.0\n",
      "4900:\t  0.01\t\t  100.0\t\t  0.04\t\t  84.0\n",
      "5000:\t  0.01\t\t  100.0\t\t  0.04\t\t  84.0\n",
      "5100:\t  0.01\t\t  100.0\t\t  0.04\t\t  84.0\n",
      "5200:\t  0.01\t\t  100.0\t\t  0.04\t\t  84.0\n",
      "5300:\t  0.01\t\t  100.0\t\t  0.04\t\t  84.0\n",
      "5400:\t  0.01\t\t  100.0\t\t  0.04\t\t  84.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-8ba4012cc44d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m\"\\ttrain_loss \\ttrain_acc \\tvalid_loss \\tvalid_acc\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_train\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_gen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m>=\u001b[0m\u001b[0mITERATIONS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-b1cb4fc3b978>\u001b[0m in \u001b[0;36mgen_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'textures'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'textures'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m                 \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'images'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'images'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m                 \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ts'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0monehot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ts'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'float32'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m                 \u001b[0mi\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/dist-packages/numpy/core/numeric.pyc\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m     \"\"\"\n\u001b[0;32m--> 460\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0masanyarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Training Loop\n",
    "BATCH_SIZE = 64\n",
    "ITERATIONS = 1e4\n",
    "VALIDATION_SIZE = 0.1 # 0.1 is ~ 100 samples for valition\n",
    "SEED = 42\n",
    "DROPOUT = True\n",
    "LEARNING_RATE = 0.0000005\n",
    "VALID_EVERY = 100\n",
    "\n",
    "batch_gen = batch_generator(data, batch_size=BATCH_SIZE, num_classes=NUM_CLASSES,\n",
    "                            num_iterations=ITERATIONS, seed=SEED, val_size=VALIDATION_SIZE)\n",
    "\n",
    "train_loss = []\n",
    "train_acc = []\n",
    "print \"\\ttrain_loss \\ttrain_acc \\tvalid_loss \\tvalid_acc\"\n",
    "for i, batch_train in enumerate(batch_gen.gen_train()):\n",
    "    if i>=ITERATIONS:\n",
    "        break\n",
    "    fetches_train = [train_op, loss, accuracy]\n",
    "    feed_dict_train = {\n",
    "        x_image_pl: batch_train['images'],\n",
    "        x_margin_pl: batch_train['margins'],\n",
    "        x_shape_pl: batch_train['shapes'],\n",
    "        x_texture_pl: batch_train['textures'],\n",
    "        ts_pl: batch_train['ts'],\n",
    "        is_training_pl: DROPOUT,\n",
    "        lr_pl: LEARNING_RATE\n",
    "    }\n",
    "    res_train = sess.run(fetches=fetches_train, feed_dict=feed_dict_train)\n",
    "    train_loss.append(res_train[1])\n",
    "    train_acc.append(res_train[2])\n",
    "    \n",
    "    if i % VALID_EVERY == 0:\n",
    "        cur_acc = 0\n",
    "        cur_loss = 0\n",
    "        tot_num = 0\n",
    "        for batch_valid, num in batch_gen.gen_valid():\n",
    "            fetches_valid = [loss, accuracy]\n",
    "            feed_dict_valid = {\n",
    "                x_image_pl: batch_valid['images'],\n",
    "                x_margin_pl: batch_valid['margins'],\n",
    "                x_shape_pl: batch_valid['shapes'],\n",
    "                x_texture_pl: batch_valid['textures'],\n",
    "                ts_pl: batch_valid['ts'],\n",
    "                is_training_pl: False,\n",
    "            }\n",
    "            res_valid = sess.run(fetches=fetches_valid, feed_dict=feed_dict_valid)\n",
    "            cur_loss += res_valid[0]*num\n",
    "            cur_acc += res_valid[1]*num\n",
    "            tot_num += num\n",
    "        valid_loss = cur_loss / float(tot_num)\n",
    "        valid_acc = (cur_acc / float(tot_num)) * 100\n",
    "        train_loss = sum(train_loss) / float(len(train_loss))\n",
    "        train_acc = sum(train_acc) / float(len(train_acc)) * 100\n",
    "        print \"%d:\\t  %.2f\\t\\t  %.1f\\t\\t  %.2f\\t\\t  %.1f\" % (i, train_loss, train_acc, valid_loss, valid_acc)\n",
    "        train_loss = []\n",
    "        train_acc = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission to Kaggle\n",
    "\n",
    "First we have to make testset predictions, then we have to place it in the submission file and the upload to kaggle for our score! You can upload at max 5 submissions a day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# GET PREDICTIONS\n",
    "# containers to collect ids and predictions\n",
    "ids_test = []\n",
    "preds_test = []\n",
    "# run like with validation\n",
    "for batch_test, num in batch_gen.gen_test():\n",
    "    # fetching for test we only need y\n",
    "    fetches_test = [y]\n",
    "    # same as validation, but no batch['ts']\n",
    "    feed_dict_test = {\n",
    "        x_image_pl: batch_test['images'],\n",
    "        x_margin_pl: batch_test['margins'],\n",
    "        x_shape_pl: batch_test['shapes'],\n",
    "        x_texture_pl: batch_test['textures'],\n",
    "        is_training_pl: False\n",
    "    }\n",
    "    # get the result\n",
    "    res_test = sess.run(fetches=fetches_test, feed_dict=feed_dict_test)\n",
    "    y_out = res_test[0]\n",
    "    ids_test.append(batch_test['ids'])\n",
    "    if num!=len(y_out):\n",
    "        # in case of the last batch, num will be less than batch_size\n",
    "        y_out = y_out[:num]\n",
    "    preds_test.append(y_out)\n",
    "# concatenate it all, to form one list/array\n",
    "ids_test = list(itertools.chain.from_iterable(ids_test))\n",
    "preds_test = np.concatenate(preds_test, axis=0)\n",
    "assert len(ids_test) == len(preds_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Acer_Capillipes</th>\n",
       "      <th>Acer_Circinatum</th>\n",
       "      <th>Acer_Mono</th>\n",
       "      <th>Acer_Opalus</th>\n",
       "      <th>Acer_Palmatum</th>\n",
       "      <th>Acer_Pictum</th>\n",
       "      <th>Acer_Platanoids</th>\n",
       "      <th>Acer_Rubrum</th>\n",
       "      <th>Acer_Rufinerve</th>\n",
       "      <th>...</th>\n",
       "      <th>Salix_Fragilis</th>\n",
       "      <th>Salix_Intergra</th>\n",
       "      <th>Sorbus_Aria</th>\n",
       "      <th>Tilia_Oliveri</th>\n",
       "      <th>Tilia_Platyphyllos</th>\n",
       "      <th>Tilia_Tomentosa</th>\n",
       "      <th>Ulmus_Bergmanniana</th>\n",
       "      <th>Viburnum_Tinus</th>\n",
       "      <th>Viburnum_x_Rhytidophylloides</th>\n",
       "      <th>Zelkova_Serrata</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>5.100685e-06</td>\n",
       "      <td>2.412410e-06</td>\n",
       "      <td>5.877471e-07</td>\n",
       "      <td>1.937902e-03</td>\n",
       "      <td>9.898745e-07</td>\n",
       "      <td>1.415328e-07</td>\n",
       "      <td>1.922456e-05</td>\n",
       "      <td>1.263543e-06</td>\n",
       "      <td>5.693773e-07</td>\n",
       "      <td>...</td>\n",
       "      <td>1.015428e-06</td>\n",
       "      <td>4.848693e-04</td>\n",
       "      <td>8.610005e-06</td>\n",
       "      <td>2.287367e-06</td>\n",
       "      <td>6.877683e-05</td>\n",
       "      <td>1.113913e-06</td>\n",
       "      <td>1.810364e-07</td>\n",
       "      <td>2.309414e-06</td>\n",
       "      <td>1.934680e-06</td>\n",
       "      <td>6.987196e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>4.312378e-06</td>\n",
       "      <td>1.142324e-05</td>\n",
       "      <td>2.505422e-05</td>\n",
       "      <td>2.276995e-04</td>\n",
       "      <td>1.421900e-05</td>\n",
       "      <td>1.741409e-05</td>\n",
       "      <td>3.877239e-04</td>\n",
       "      <td>1.598168e-05</td>\n",
       "      <td>2.164532e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>3.267626e-06</td>\n",
       "      <td>2.513821e-05</td>\n",
       "      <td>9.021912e-06</td>\n",
       "      <td>1.609234e-05</td>\n",
       "      <td>5.398817e-06</td>\n",
       "      <td>1.122376e-04</td>\n",
       "      <td>1.609923e-06</td>\n",
       "      <td>2.098621e-04</td>\n",
       "      <td>3.769313e-06</td>\n",
       "      <td>2.246706e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9</td>\n",
       "      <td>3.103504e-05</td>\n",
       "      <td>9.649146e-01</td>\n",
       "      <td>2.428891e-05</td>\n",
       "      <td>3.780332e-05</td>\n",
       "      <td>6.383263e-03</td>\n",
       "      <td>6.635626e-05</td>\n",
       "      <td>8.623439e-06</td>\n",
       "      <td>5.011155e-04</td>\n",
       "      <td>3.080307e-03</td>\n",
       "      <td>...</td>\n",
       "      <td>7.580579e-05</td>\n",
       "      <td>1.971487e-05</td>\n",
       "      <td>2.262111e-05</td>\n",
       "      <td>5.601712e-06</td>\n",
       "      <td>8.572335e-07</td>\n",
       "      <td>1.320894e-05</td>\n",
       "      <td>5.410352e-05</td>\n",
       "      <td>7.606266e-07</td>\n",
       "      <td>9.855883e-07</td>\n",
       "      <td>7.037049e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12</td>\n",
       "      <td>2.556006e-06</td>\n",
       "      <td>1.693535e-03</td>\n",
       "      <td>3.073582e-06</td>\n",
       "      <td>2.032537e-06</td>\n",
       "      <td>1.603946e-05</td>\n",
       "      <td>8.469344e-07</td>\n",
       "      <td>2.134096e-04</td>\n",
       "      <td>4.118432e-05</td>\n",
       "      <td>1.491737e-03</td>\n",
       "      <td>...</td>\n",
       "      <td>1.048205e-04</td>\n",
       "      <td>2.698628e-06</td>\n",
       "      <td>3.979252e-04</td>\n",
       "      <td>8.748960e-06</td>\n",
       "      <td>1.894345e-04</td>\n",
       "      <td>7.730371e-04</td>\n",
       "      <td>4.923149e-02</td>\n",
       "      <td>7.229227e-06</td>\n",
       "      <td>2.527448e-05</td>\n",
       "      <td>2.818535e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13</td>\n",
       "      <td>6.189646e-06</td>\n",
       "      <td>2.715071e-05</td>\n",
       "      <td>1.032662e-07</td>\n",
       "      <td>5.691051e-07</td>\n",
       "      <td>7.512084e-06</td>\n",
       "      <td>7.799451e-08</td>\n",
       "      <td>3.097986e-05</td>\n",
       "      <td>2.140760e-06</td>\n",
       "      <td>1.722694e-04</td>\n",
       "      <td>...</td>\n",
       "      <td>7.931955e-05</td>\n",
       "      <td>2.736775e-07</td>\n",
       "      <td>2.613650e-04</td>\n",
       "      <td>7.346861e-06</td>\n",
       "      <td>9.931298e-04</td>\n",
       "      <td>1.977421e-04</td>\n",
       "      <td>5.775944e-04</td>\n",
       "      <td>1.175690e-05</td>\n",
       "      <td>7.660232e-06</td>\n",
       "      <td>5.087229e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>16</td>\n",
       "      <td>3.277398e-04</td>\n",
       "      <td>8.372749e-05</td>\n",
       "      <td>5.893797e-05</td>\n",
       "      <td>6.800114e-01</td>\n",
       "      <td>5.904526e-05</td>\n",
       "      <td>1.395774e-05</td>\n",
       "      <td>2.266977e-04</td>\n",
       "      <td>5.485127e-03</td>\n",
       "      <td>1.992310e-04</td>\n",
       "      <td>...</td>\n",
       "      <td>1.304590e-03</td>\n",
       "      <td>6.563566e-04</td>\n",
       "      <td>1.259644e-03</td>\n",
       "      <td>1.743636e-03</td>\n",
       "      <td>3.731517e-03</td>\n",
       "      <td>2.040616e-03</td>\n",
       "      <td>2.223019e-04</td>\n",
       "      <td>3.221096e-04</td>\n",
       "      <td>5.891598e-03</td>\n",
       "      <td>6.087327e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>19</td>\n",
       "      <td>1.448475e-04</td>\n",
       "      <td>5.999024e-05</td>\n",
       "      <td>3.009324e-05</td>\n",
       "      <td>9.600083e-01</td>\n",
       "      <td>1.400709e-05</td>\n",
       "      <td>5.621324e-06</td>\n",
       "      <td>5.499132e-05</td>\n",
       "      <td>3.559266e-05</td>\n",
       "      <td>1.087574e-05</td>\n",
       "      <td>...</td>\n",
       "      <td>1.444610e-05</td>\n",
       "      <td>2.158408e-04</td>\n",
       "      <td>1.989583e-04</td>\n",
       "      <td>1.188127e-04</td>\n",
       "      <td>4.468976e-04</td>\n",
       "      <td>7.788631e-04</td>\n",
       "      <td>1.071679e-05</td>\n",
       "      <td>1.625313e-04</td>\n",
       "      <td>2.955268e-04</td>\n",
       "      <td>4.634736e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>23</td>\n",
       "      <td>3.171916e-07</td>\n",
       "      <td>7.500980e-05</td>\n",
       "      <td>8.480211e-05</td>\n",
       "      <td>5.348376e-05</td>\n",
       "      <td>1.010804e-05</td>\n",
       "      <td>1.988761e-03</td>\n",
       "      <td>3.183402e-05</td>\n",
       "      <td>2.463548e-07</td>\n",
       "      <td>3.895387e-08</td>\n",
       "      <td>...</td>\n",
       "      <td>9.946294e-07</td>\n",
       "      <td>4.765113e-05</td>\n",
       "      <td>1.489128e-06</td>\n",
       "      <td>6.503938e-07</td>\n",
       "      <td>1.352986e-05</td>\n",
       "      <td>1.678264e-07</td>\n",
       "      <td>1.720844e-07</td>\n",
       "      <td>2.727328e-05</td>\n",
       "      <td>2.883907e-05</td>\n",
       "      <td>1.743627e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>24</td>\n",
       "      <td>1.640448e-04</td>\n",
       "      <td>1.104876e-05</td>\n",
       "      <td>9.477199e-06</td>\n",
       "      <td>6.789436e-08</td>\n",
       "      <td>1.030943e-05</td>\n",
       "      <td>1.476463e-04</td>\n",
       "      <td>1.856878e-04</td>\n",
       "      <td>3.971901e-06</td>\n",
       "      <td>3.189352e-04</td>\n",
       "      <td>...</td>\n",
       "      <td>2.750560e-06</td>\n",
       "      <td>1.884413e-06</td>\n",
       "      <td>6.784715e-07</td>\n",
       "      <td>2.140089e-05</td>\n",
       "      <td>2.194002e-06</td>\n",
       "      <td>3.977391e-07</td>\n",
       "      <td>1.224768e-07</td>\n",
       "      <td>2.685703e-08</td>\n",
       "      <td>1.279548e-07</td>\n",
       "      <td>2.141610e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>28</td>\n",
       "      <td>1.466255e-03</td>\n",
       "      <td>1.072608e-03</td>\n",
       "      <td>2.352958e-05</td>\n",
       "      <td>5.261607e-06</td>\n",
       "      <td>3.463529e-05</td>\n",
       "      <td>1.082120e-05</td>\n",
       "      <td>7.083551e-05</td>\n",
       "      <td>3.812846e-04</td>\n",
       "      <td>9.893776e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>2.031515e-04</td>\n",
       "      <td>1.541470e-05</td>\n",
       "      <td>1.895658e-04</td>\n",
       "      <td>1.781775e-05</td>\n",
       "      <td>1.126358e-04</td>\n",
       "      <td>1.031415e-03</td>\n",
       "      <td>2.078585e-04</td>\n",
       "      <td>7.360682e-08</td>\n",
       "      <td>1.073095e-06</td>\n",
       "      <td>1.580574e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>33</td>\n",
       "      <td>1.710005e-05</td>\n",
       "      <td>2.876335e-06</td>\n",
       "      <td>1.148133e-04</td>\n",
       "      <td>3.329956e-04</td>\n",
       "      <td>1.802546e-05</td>\n",
       "      <td>7.363225e-05</td>\n",
       "      <td>4.020051e-05</td>\n",
       "      <td>2.779674e-06</td>\n",
       "      <td>9.358470e-08</td>\n",
       "      <td>...</td>\n",
       "      <td>4.902801e-05</td>\n",
       "      <td>2.559539e-05</td>\n",
       "      <td>7.996816e-05</td>\n",
       "      <td>1.180693e-03</td>\n",
       "      <td>1.629108e-05</td>\n",
       "      <td>6.567297e-08</td>\n",
       "      <td>1.172341e-04</td>\n",
       "      <td>7.284094e-03</td>\n",
       "      <td>1.500942e-02</td>\n",
       "      <td>6.116187e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>36</td>\n",
       "      <td>2.169724e-03</td>\n",
       "      <td>1.590941e-05</td>\n",
       "      <td>3.369599e-05</td>\n",
       "      <td>7.622061e-06</td>\n",
       "      <td>9.362630e-05</td>\n",
       "      <td>9.973985e-06</td>\n",
       "      <td>5.153053e-06</td>\n",
       "      <td>2.001973e-05</td>\n",
       "      <td>1.381119e-04</td>\n",
       "      <td>...</td>\n",
       "      <td>2.137712e-06</td>\n",
       "      <td>3.481058e-06</td>\n",
       "      <td>4.724215e-05</td>\n",
       "      <td>8.799047e-06</td>\n",
       "      <td>5.689618e-05</td>\n",
       "      <td>1.586029e-06</td>\n",
       "      <td>6.356699e-06</td>\n",
       "      <td>3.218537e-05</td>\n",
       "      <td>3.966579e-06</td>\n",
       "      <td>1.848037e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>39</td>\n",
       "      <td>2.837284e-04</td>\n",
       "      <td>2.425320e-07</td>\n",
       "      <td>2.382951e-05</td>\n",
       "      <td>5.111489e-05</td>\n",
       "      <td>6.774516e-07</td>\n",
       "      <td>8.899924e-07</td>\n",
       "      <td>1.507555e-07</td>\n",
       "      <td>1.619520e-06</td>\n",
       "      <td>2.468836e-07</td>\n",
       "      <td>...</td>\n",
       "      <td>6.718647e-06</td>\n",
       "      <td>3.426693e-04</td>\n",
       "      <td>2.673141e-07</td>\n",
       "      <td>2.583342e-04</td>\n",
       "      <td>1.809465e-06</td>\n",
       "      <td>5.001092e-07</td>\n",
       "      <td>6.828984e-07</td>\n",
       "      <td>4.173997e-05</td>\n",
       "      <td>3.865789e-06</td>\n",
       "      <td>8.141697e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>41</td>\n",
       "      <td>2.684766e-05</td>\n",
       "      <td>2.544118e-06</td>\n",
       "      <td>1.507194e-03</td>\n",
       "      <td>4.238139e-05</td>\n",
       "      <td>1.368411e-06</td>\n",
       "      <td>1.033137e-05</td>\n",
       "      <td>2.483299e-05</td>\n",
       "      <td>1.146267e-05</td>\n",
       "      <td>2.548856e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>2.957194e-05</td>\n",
       "      <td>3.907455e-05</td>\n",
       "      <td>1.803960e-06</td>\n",
       "      <td>4.938464e-04</td>\n",
       "      <td>1.045530e-05</td>\n",
       "      <td>7.566721e-03</td>\n",
       "      <td>3.108142e-06</td>\n",
       "      <td>4.276371e-07</td>\n",
       "      <td>6.408330e-06</td>\n",
       "      <td>5.493495e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>44</td>\n",
       "      <td>1.674098e-07</td>\n",
       "      <td>1.609267e-07</td>\n",
       "      <td>9.203021e-08</td>\n",
       "      <td>2.505952e-04</td>\n",
       "      <td>1.071842e-06</td>\n",
       "      <td>2.432817e-07</td>\n",
       "      <td>9.574599e-07</td>\n",
       "      <td>2.764120e-06</td>\n",
       "      <td>3.545994e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>5.824529e-06</td>\n",
       "      <td>1.348715e-06</td>\n",
       "      <td>5.016460e-07</td>\n",
       "      <td>2.945733e-05</td>\n",
       "      <td>6.780209e-06</td>\n",
       "      <td>4.380234e-06</td>\n",
       "      <td>1.526923e-07</td>\n",
       "      <td>6.336413e-04</td>\n",
       "      <td>1.217707e-07</td>\n",
       "      <td>2.608506e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>46</td>\n",
       "      <td>1.562715e-05</td>\n",
       "      <td>1.402860e-04</td>\n",
       "      <td>2.414944e-05</td>\n",
       "      <td>3.654345e-03</td>\n",
       "      <td>6.076969e-06</td>\n",
       "      <td>7.852581e-06</td>\n",
       "      <td>2.972598e-04</td>\n",
       "      <td>2.402261e-06</td>\n",
       "      <td>2.987228e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>7.660515e-06</td>\n",
       "      <td>7.035894e-06</td>\n",
       "      <td>7.596071e-03</td>\n",
       "      <td>3.674875e-04</td>\n",
       "      <td>3.443590e-04</td>\n",
       "      <td>1.203780e-04</td>\n",
       "      <td>2.609295e-03</td>\n",
       "      <td>4.156264e-04</td>\n",
       "      <td>2.332870e-04</td>\n",
       "      <td>3.216509e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>47</td>\n",
       "      <td>1.207388e-05</td>\n",
       "      <td>2.264785e-05</td>\n",
       "      <td>9.871576e-04</td>\n",
       "      <td>2.488431e-07</td>\n",
       "      <td>9.085968e-06</td>\n",
       "      <td>7.195359e-05</td>\n",
       "      <td>1.651239e-03</td>\n",
       "      <td>2.449073e-06</td>\n",
       "      <td>4.065607e-04</td>\n",
       "      <td>...</td>\n",
       "      <td>7.360163e-07</td>\n",
       "      <td>2.649848e-05</td>\n",
       "      <td>5.025420e-06</td>\n",
       "      <td>2.132082e-05</td>\n",
       "      <td>2.053418e-06</td>\n",
       "      <td>1.947693e-07</td>\n",
       "      <td>1.219362e-07</td>\n",
       "      <td>9.635706e-08</td>\n",
       "      <td>3.977082e-07</td>\n",
       "      <td>1.263164e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>51</td>\n",
       "      <td>5.045758e-05</td>\n",
       "      <td>3.254994e-05</td>\n",
       "      <td>5.993143e-03</td>\n",
       "      <td>6.478239e-05</td>\n",
       "      <td>3.498209e-05</td>\n",
       "      <td>2.599840e-04</td>\n",
       "      <td>7.917164e-05</td>\n",
       "      <td>2.176974e-03</td>\n",
       "      <td>1.964623e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>6.204216e-05</td>\n",
       "      <td>6.928353e-04</td>\n",
       "      <td>2.390751e-06</td>\n",
       "      <td>2.470055e-04</td>\n",
       "      <td>2.457492e-06</td>\n",
       "      <td>7.799804e-05</td>\n",
       "      <td>3.589343e-06</td>\n",
       "      <td>3.409938e-06</td>\n",
       "      <td>8.829691e-05</td>\n",
       "      <td>1.170716e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>52</td>\n",
       "      <td>6.458659e-05</td>\n",
       "      <td>7.858984e-05</td>\n",
       "      <td>6.909396e-06</td>\n",
       "      <td>1.846784e-05</td>\n",
       "      <td>2.509887e-05</td>\n",
       "      <td>6.017070e-06</td>\n",
       "      <td>1.256750e-04</td>\n",
       "      <td>1.909471e-02</td>\n",
       "      <td>2.828155e-04</td>\n",
       "      <td>...</td>\n",
       "      <td>1.061772e-03</td>\n",
       "      <td>3.933068e-06</td>\n",
       "      <td>1.660814e-04</td>\n",
       "      <td>9.800599e-03</td>\n",
       "      <td>9.397652e-05</td>\n",
       "      <td>1.210757e-03</td>\n",
       "      <td>8.812274e-04</td>\n",
       "      <td>7.285024e-06</td>\n",
       "      <td>8.167784e-06</td>\n",
       "      <td>1.133546e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>53</td>\n",
       "      <td>3.303559e-05</td>\n",
       "      <td>5.470498e-06</td>\n",
       "      <td>8.638493e-08</td>\n",
       "      <td>1.045149e-06</td>\n",
       "      <td>7.204713e-06</td>\n",
       "      <td>1.689403e-04</td>\n",
       "      <td>3.395687e-05</td>\n",
       "      <td>6.433523e-07</td>\n",
       "      <td>1.694212e-07</td>\n",
       "      <td>...</td>\n",
       "      <td>5.804612e-06</td>\n",
       "      <td>1.711523e-05</td>\n",
       "      <td>4.206014e-07</td>\n",
       "      <td>1.829545e-05</td>\n",
       "      <td>6.195688e-06</td>\n",
       "      <td>1.919363e-07</td>\n",
       "      <td>7.788410e-07</td>\n",
       "      <td>3.991335e-08</td>\n",
       "      <td>2.020168e-07</td>\n",
       "      <td>3.430661e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>57</td>\n",
       "      <td>1.720937e-07</td>\n",
       "      <td>5.862953e-08</td>\n",
       "      <td>3.033212e-05</td>\n",
       "      <td>2.121668e-08</td>\n",
       "      <td>7.182595e-07</td>\n",
       "      <td>1.384003e-06</td>\n",
       "      <td>8.721802e-09</td>\n",
       "      <td>1.091787e-08</td>\n",
       "      <td>1.002931e-09</td>\n",
       "      <td>...</td>\n",
       "      <td>2.828729e-07</td>\n",
       "      <td>4.796914e-04</td>\n",
       "      <td>6.366831e-09</td>\n",
       "      <td>2.249857e-08</td>\n",
       "      <td>1.047165e-06</td>\n",
       "      <td>1.084100e-09</td>\n",
       "      <td>1.520351e-08</td>\n",
       "      <td>1.043932e-03</td>\n",
       "      <td>1.633731e-06</td>\n",
       "      <td>1.040544e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>59</td>\n",
       "      <td>3.919239e-04</td>\n",
       "      <td>2.196559e-06</td>\n",
       "      <td>5.180858e-03</td>\n",
       "      <td>4.802334e-05</td>\n",
       "      <td>2.717825e-05</td>\n",
       "      <td>1.060547e-05</td>\n",
       "      <td>2.782068e-06</td>\n",
       "      <td>3.496424e-06</td>\n",
       "      <td>3.371314e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>2.769726e-06</td>\n",
       "      <td>7.730975e-04</td>\n",
       "      <td>4.363312e-06</td>\n",
       "      <td>9.274812e-05</td>\n",
       "      <td>6.067779e-05</td>\n",
       "      <td>8.523924e-06</td>\n",
       "      <td>1.615564e-06</td>\n",
       "      <td>1.487565e-03</td>\n",
       "      <td>2.856674e-04</td>\n",
       "      <td>1.136834e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>62</td>\n",
       "      <td>1.849914e-10</td>\n",
       "      <td>5.092108e-09</td>\n",
       "      <td>2.728949e-09</td>\n",
       "      <td>1.389164e-08</td>\n",
       "      <td>1.050887e-08</td>\n",
       "      <td>2.051834e-08</td>\n",
       "      <td>6.172512e-08</td>\n",
       "      <td>2.240932e-08</td>\n",
       "      <td>7.793210e-11</td>\n",
       "      <td>...</td>\n",
       "      <td>2.078816e-06</td>\n",
       "      <td>1.779209e-09</td>\n",
       "      <td>4.567689e-09</td>\n",
       "      <td>1.976186e-08</td>\n",
       "      <td>4.489929e-10</td>\n",
       "      <td>8.343788e-10</td>\n",
       "      <td>2.393548e-09</td>\n",
       "      <td>1.763315e-07</td>\n",
       "      <td>1.067737e-08</td>\n",
       "      <td>4.571497e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>65</td>\n",
       "      <td>7.326046e-09</td>\n",
       "      <td>1.146629e-07</td>\n",
       "      <td>5.490738e-06</td>\n",
       "      <td>6.736457e-08</td>\n",
       "      <td>1.160043e-07</td>\n",
       "      <td>9.411501e-08</td>\n",
       "      <td>5.197959e-04</td>\n",
       "      <td>8.424094e-09</td>\n",
       "      <td>2.776460e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>1.101705e-06</td>\n",
       "      <td>9.585277e-07</td>\n",
       "      <td>8.679514e-06</td>\n",
       "      <td>2.564097e-07</td>\n",
       "      <td>9.328999e-05</td>\n",
       "      <td>6.294757e-07</td>\n",
       "      <td>2.299215e-04</td>\n",
       "      <td>5.658289e-04</td>\n",
       "      <td>2.393929e-03</td>\n",
       "      <td>2.010694e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>68</td>\n",
       "      <td>6.936047e-06</td>\n",
       "      <td>9.891200e-01</td>\n",
       "      <td>2.407083e-06</td>\n",
       "      <td>2.343225e-06</td>\n",
       "      <td>9.198199e-05</td>\n",
       "      <td>2.759043e-06</td>\n",
       "      <td>8.491110e-06</td>\n",
       "      <td>4.936474e-05</td>\n",
       "      <td>1.598945e-04</td>\n",
       "      <td>...</td>\n",
       "      <td>3.235514e-05</td>\n",
       "      <td>1.792711e-05</td>\n",
       "      <td>1.247719e-05</td>\n",
       "      <td>1.321841e-06</td>\n",
       "      <td>2.295438e-06</td>\n",
       "      <td>3.918069e-05</td>\n",
       "      <td>1.490536e-04</td>\n",
       "      <td>2.371046e-07</td>\n",
       "      <td>7.541565e-07</td>\n",
       "      <td>3.309535e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>70</td>\n",
       "      <td>3.540725e-06</td>\n",
       "      <td>5.957349e-06</td>\n",
       "      <td>2.062797e-04</td>\n",
       "      <td>3.795326e-06</td>\n",
       "      <td>1.154682e-05</td>\n",
       "      <td>1.220005e-04</td>\n",
       "      <td>4.646279e-06</td>\n",
       "      <td>6.024828e-07</td>\n",
       "      <td>7.160270e-07</td>\n",
       "      <td>...</td>\n",
       "      <td>1.119259e-06</td>\n",
       "      <td>3.767865e-05</td>\n",
       "      <td>6.733817e-06</td>\n",
       "      <td>1.992526e-06</td>\n",
       "      <td>1.487477e-05</td>\n",
       "      <td>1.354031e-07</td>\n",
       "      <td>4.032670e-06</td>\n",
       "      <td>6.040760e-04</td>\n",
       "      <td>8.687493e-05</td>\n",
       "      <td>1.304432e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>74</td>\n",
       "      <td>1.316960e-08</td>\n",
       "      <td>2.368121e-06</td>\n",
       "      <td>4.863270e-06</td>\n",
       "      <td>1.213677e-05</td>\n",
       "      <td>2.530349e-06</td>\n",
       "      <td>1.810221e-04</td>\n",
       "      <td>9.473308e-06</td>\n",
       "      <td>4.328331e-07</td>\n",
       "      <td>3.592488e-08</td>\n",
       "      <td>...</td>\n",
       "      <td>9.658292e-06</td>\n",
       "      <td>2.263426e-06</td>\n",
       "      <td>4.335928e-07</td>\n",
       "      <td>2.112389e-05</td>\n",
       "      <td>6.346430e-07</td>\n",
       "      <td>3.475899e-07</td>\n",
       "      <td>1.326052e-06</td>\n",
       "      <td>2.264514e-04</td>\n",
       "      <td>4.529306e-06</td>\n",
       "      <td>3.558214e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>77</td>\n",
       "      <td>1.609329e-11</td>\n",
       "      <td>1.666645e-07</td>\n",
       "      <td>1.330036e-08</td>\n",
       "      <td>7.304642e-07</td>\n",
       "      <td>2.370336e-08</td>\n",
       "      <td>1.429611e-06</td>\n",
       "      <td>9.244211e-06</td>\n",
       "      <td>4.426106e-09</td>\n",
       "      <td>3.221968e-09</td>\n",
       "      <td>...</td>\n",
       "      <td>1.763902e-08</td>\n",
       "      <td>6.098416e-09</td>\n",
       "      <td>6.239500e-09</td>\n",
       "      <td>6.639810e-09</td>\n",
       "      <td>3.778765e-09</td>\n",
       "      <td>5.516613e-08</td>\n",
       "      <td>3.719345e-08</td>\n",
       "      <td>5.281830e-08</td>\n",
       "      <td>5.918338e-08</td>\n",
       "      <td>1.403673e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>79</td>\n",
       "      <td>3.597878e-04</td>\n",
       "      <td>4.525925e-06</td>\n",
       "      <td>5.259288e-05</td>\n",
       "      <td>6.205271e-07</td>\n",
       "      <td>4.145176e-05</td>\n",
       "      <td>4.521232e-04</td>\n",
       "      <td>9.294804e-07</td>\n",
       "      <td>2.173182e-07</td>\n",
       "      <td>1.206652e-05</td>\n",
       "      <td>...</td>\n",
       "      <td>1.881741e-06</td>\n",
       "      <td>1.801210e-05</td>\n",
       "      <td>4.391960e-06</td>\n",
       "      <td>4.830018e-06</td>\n",
       "      <td>7.892981e-07</td>\n",
       "      <td>1.913674e-08</td>\n",
       "      <td>4.143508e-07</td>\n",
       "      <td>4.020494e-09</td>\n",
       "      <td>2.639540e-05</td>\n",
       "      <td>3.201887e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>86</td>\n",
       "      <td>1.329394e-06</td>\n",
       "      <td>4.409827e-07</td>\n",
       "      <td>2.187596e-05</td>\n",
       "      <td>7.258432e-07</td>\n",
       "      <td>2.218561e-06</td>\n",
       "      <td>3.322652e-05</td>\n",
       "      <td>2.369600e-06</td>\n",
       "      <td>8.624168e-08</td>\n",
       "      <td>4.286561e-07</td>\n",
       "      <td>...</td>\n",
       "      <td>1.410489e-07</td>\n",
       "      <td>1.616385e-06</td>\n",
       "      <td>9.609012e-07</td>\n",
       "      <td>3.439576e-07</td>\n",
       "      <td>1.101925e-06</td>\n",
       "      <td>2.532985e-08</td>\n",
       "      <td>2.955292e-07</td>\n",
       "      <td>1.051313e-04</td>\n",
       "      <td>3.253613e-05</td>\n",
       "      <td>8.386794e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>1493</td>\n",
       "      <td>3.031251e-05</td>\n",
       "      <td>2.678755e-03</td>\n",
       "      <td>1.662585e-04</td>\n",
       "      <td>3.174586e-05</td>\n",
       "      <td>3.930589e-04</td>\n",
       "      <td>9.402625e-04</td>\n",
       "      <td>6.509766e-04</td>\n",
       "      <td>5.307657e-05</td>\n",
       "      <td>1.173775e-05</td>\n",
       "      <td>...</td>\n",
       "      <td>1.122894e-03</td>\n",
       "      <td>8.123437e-05</td>\n",
       "      <td>4.250840e-03</td>\n",
       "      <td>2.020107e-03</td>\n",
       "      <td>1.089542e-04</td>\n",
       "      <td>1.398945e-05</td>\n",
       "      <td>1.063141e-02</td>\n",
       "      <td>1.721183e-03</td>\n",
       "      <td>4.969870e-04</td>\n",
       "      <td>7.579166e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>1495</td>\n",
       "      <td>1.318739e-06</td>\n",
       "      <td>5.894526e-07</td>\n",
       "      <td>3.799069e-05</td>\n",
       "      <td>2.246228e-06</td>\n",
       "      <td>2.753388e-06</td>\n",
       "      <td>1.666475e-06</td>\n",
       "      <td>4.043600e-06</td>\n",
       "      <td>2.698249e-04</td>\n",
       "      <td>1.813196e-07</td>\n",
       "      <td>...</td>\n",
       "      <td>4.632004e-05</td>\n",
       "      <td>2.314743e-05</td>\n",
       "      <td>1.319309e-07</td>\n",
       "      <td>9.819421e-01</td>\n",
       "      <td>2.434656e-06</td>\n",
       "      <td>2.250847e-06</td>\n",
       "      <td>7.040683e-07</td>\n",
       "      <td>5.364548e-06</td>\n",
       "      <td>1.515567e-06</td>\n",
       "      <td>2.494497e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>1497</td>\n",
       "      <td>6.955544e-03</td>\n",
       "      <td>1.067015e-02</td>\n",
       "      <td>5.447153e-06</td>\n",
       "      <td>2.407807e-04</td>\n",
       "      <td>9.688936e-04</td>\n",
       "      <td>1.117147e-05</td>\n",
       "      <td>2.433453e-05</td>\n",
       "      <td>3.297738e-04</td>\n",
       "      <td>1.704964e-02</td>\n",
       "      <td>...</td>\n",
       "      <td>1.852340e-03</td>\n",
       "      <td>2.580321e-05</td>\n",
       "      <td>1.486365e-03</td>\n",
       "      <td>8.562071e-05</td>\n",
       "      <td>1.202832e-02</td>\n",
       "      <td>2.199880e-03</td>\n",
       "      <td>6.704818e-03</td>\n",
       "      <td>1.065958e-05</td>\n",
       "      <td>2.095960e-05</td>\n",
       "      <td>2.428982e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>1498</td>\n",
       "      <td>8.132478e-08</td>\n",
       "      <td>1.132570e-06</td>\n",
       "      <td>1.779398e-05</td>\n",
       "      <td>1.368421e-05</td>\n",
       "      <td>8.691900e-07</td>\n",
       "      <td>7.981706e-07</td>\n",
       "      <td>1.261864e-05</td>\n",
       "      <td>8.118120e-06</td>\n",
       "      <td>8.602336e-07</td>\n",
       "      <td>...</td>\n",
       "      <td>3.492281e-05</td>\n",
       "      <td>1.924845e-05</td>\n",
       "      <td>1.341229e-06</td>\n",
       "      <td>3.806033e-04</td>\n",
       "      <td>3.704371e-06</td>\n",
       "      <td>7.969818e-07</td>\n",
       "      <td>1.285498e-05</td>\n",
       "      <td>1.977723e-04</td>\n",
       "      <td>9.091429e-06</td>\n",
       "      <td>1.724240e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>1503</td>\n",
       "      <td>2.424580e-06</td>\n",
       "      <td>1.151860e-06</td>\n",
       "      <td>4.220916e-06</td>\n",
       "      <td>5.204691e-07</td>\n",
       "      <td>9.024905e-07</td>\n",
       "      <td>4.478792e-06</td>\n",
       "      <td>5.061892e-06</td>\n",
       "      <td>7.543643e-08</td>\n",
       "      <td>4.444979e-07</td>\n",
       "      <td>...</td>\n",
       "      <td>3.333730e-07</td>\n",
       "      <td>6.571474e-06</td>\n",
       "      <td>2.538010e-08</td>\n",
       "      <td>1.871423e-07</td>\n",
       "      <td>8.040818e-08</td>\n",
       "      <td>7.105646e-10</td>\n",
       "      <td>2.904834e-09</td>\n",
       "      <td>4.358912e-06</td>\n",
       "      <td>1.129633e-05</td>\n",
       "      <td>5.789196e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>569</th>\n",
       "      <td>1510</td>\n",
       "      <td>2.044535e-05</td>\n",
       "      <td>4.491383e-05</td>\n",
       "      <td>9.465865e-08</td>\n",
       "      <td>1.733130e-07</td>\n",
       "      <td>1.498447e-05</td>\n",
       "      <td>1.583407e-07</td>\n",
       "      <td>1.270387e-05</td>\n",
       "      <td>4.367338e-06</td>\n",
       "      <td>3.768804e-04</td>\n",
       "      <td>...</td>\n",
       "      <td>1.299653e-04</td>\n",
       "      <td>1.908526e-07</td>\n",
       "      <td>5.652427e-04</td>\n",
       "      <td>7.709732e-06</td>\n",
       "      <td>1.114253e-04</td>\n",
       "      <td>6.898544e-05</td>\n",
       "      <td>6.618744e-04</td>\n",
       "      <td>1.248170e-05</td>\n",
       "      <td>1.289161e-05</td>\n",
       "      <td>4.290463e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>570</th>\n",
       "      <td>1513</td>\n",
       "      <td>4.152271e-04</td>\n",
       "      <td>1.126968e-04</td>\n",
       "      <td>3.095721e-06</td>\n",
       "      <td>4.596747e-03</td>\n",
       "      <td>3.315627e-05</td>\n",
       "      <td>4.419860e-07</td>\n",
       "      <td>6.062882e-05</td>\n",
       "      <td>3.664158e-05</td>\n",
       "      <td>4.884847e-04</td>\n",
       "      <td>...</td>\n",
       "      <td>1.385203e-04</td>\n",
       "      <td>5.319734e-05</td>\n",
       "      <td>8.648143e-04</td>\n",
       "      <td>4.635487e-05</td>\n",
       "      <td>9.628000e-01</td>\n",
       "      <td>6.207589e-03</td>\n",
       "      <td>4.462679e-04</td>\n",
       "      <td>3.142048e-05</td>\n",
       "      <td>9.819636e-05</td>\n",
       "      <td>7.166729e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>571</th>\n",
       "      <td>1517</td>\n",
       "      <td>2.458563e-05</td>\n",
       "      <td>4.847772e-06</td>\n",
       "      <td>7.619724e-06</td>\n",
       "      <td>5.863536e-07</td>\n",
       "      <td>4.027911e-06</td>\n",
       "      <td>3.037552e-04</td>\n",
       "      <td>4.051010e-03</td>\n",
       "      <td>7.858964e-07</td>\n",
       "      <td>1.244709e-05</td>\n",
       "      <td>...</td>\n",
       "      <td>4.038423e-06</td>\n",
       "      <td>7.368244e-06</td>\n",
       "      <td>1.003339e-05</td>\n",
       "      <td>1.812392e-05</td>\n",
       "      <td>1.039223e-04</td>\n",
       "      <td>1.330943e-06</td>\n",
       "      <td>2.411209e-06</td>\n",
       "      <td>4.176620e-07</td>\n",
       "      <td>3.782613e-06</td>\n",
       "      <td>1.748885e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>572</th>\n",
       "      <td>1522</td>\n",
       "      <td>6.305919e-06</td>\n",
       "      <td>3.478516e-06</td>\n",
       "      <td>3.914444e-06</td>\n",
       "      <td>3.993074e-04</td>\n",
       "      <td>2.193975e-06</td>\n",
       "      <td>1.184731e-06</td>\n",
       "      <td>6.706700e-07</td>\n",
       "      <td>2.535064e-05</td>\n",
       "      <td>1.790579e-05</td>\n",
       "      <td>...</td>\n",
       "      <td>7.179528e-07</td>\n",
       "      <td>1.392134e-05</td>\n",
       "      <td>1.020551e-05</td>\n",
       "      <td>1.000089e-06</td>\n",
       "      <td>2.098831e-06</td>\n",
       "      <td>2.484997e-04</td>\n",
       "      <td>5.190415e-07</td>\n",
       "      <td>3.580616e-05</td>\n",
       "      <td>5.964840e-07</td>\n",
       "      <td>6.358935e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>573</th>\n",
       "      <td>1526</td>\n",
       "      <td>5.165176e-05</td>\n",
       "      <td>1.390774e-06</td>\n",
       "      <td>4.313419e-05</td>\n",
       "      <td>7.544258e-04</td>\n",
       "      <td>1.750050e-06</td>\n",
       "      <td>6.887137e-06</td>\n",
       "      <td>1.366514e-04</td>\n",
       "      <td>1.356353e-06</td>\n",
       "      <td>1.719631e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>9.005345e-06</td>\n",
       "      <td>2.722215e-05</td>\n",
       "      <td>6.478511e-06</td>\n",
       "      <td>2.531584e-04</td>\n",
       "      <td>3.706667e-05</td>\n",
       "      <td>2.506213e-05</td>\n",
       "      <td>1.150804e-06</td>\n",
       "      <td>3.022444e-05</td>\n",
       "      <td>3.303706e-06</td>\n",
       "      <td>1.862428e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>574</th>\n",
       "      <td>1528</td>\n",
       "      <td>4.655133e-07</td>\n",
       "      <td>9.218487e-06</td>\n",
       "      <td>1.085533e-06</td>\n",
       "      <td>9.187168e-05</td>\n",
       "      <td>9.398969e-06</td>\n",
       "      <td>1.608738e-06</td>\n",
       "      <td>3.769889e-05</td>\n",
       "      <td>4.232019e-07</td>\n",
       "      <td>9.283282e-07</td>\n",
       "      <td>...</td>\n",
       "      <td>1.302681e-07</td>\n",
       "      <td>1.180830e-06</td>\n",
       "      <td>4.263813e-07</td>\n",
       "      <td>5.122200e-07</td>\n",
       "      <td>2.693186e-06</td>\n",
       "      <td>8.946493e-08</td>\n",
       "      <td>1.218531e-07</td>\n",
       "      <td>1.047218e-03</td>\n",
       "      <td>2.453055e-05</td>\n",
       "      <td>1.344052e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>575</th>\n",
       "      <td>1533</td>\n",
       "      <td>5.545036e-06</td>\n",
       "      <td>1.473832e-05</td>\n",
       "      <td>2.347347e-06</td>\n",
       "      <td>1.054847e-03</td>\n",
       "      <td>6.276145e-06</td>\n",
       "      <td>2.731647e-06</td>\n",
       "      <td>1.743846e-05</td>\n",
       "      <td>2.094332e-05</td>\n",
       "      <td>2.090858e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>8.631513e-07</td>\n",
       "      <td>4.310547e-05</td>\n",
       "      <td>9.184498e-06</td>\n",
       "      <td>8.554584e-06</td>\n",
       "      <td>1.984236e-06</td>\n",
       "      <td>1.776964e-04</td>\n",
       "      <td>1.524096e-06</td>\n",
       "      <td>7.978188e-05</td>\n",
       "      <td>9.461069e-07</td>\n",
       "      <td>7.704433e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>576</th>\n",
       "      <td>1534</td>\n",
       "      <td>9.334250e-05</td>\n",
       "      <td>1.520183e-04</td>\n",
       "      <td>2.370013e-06</td>\n",
       "      <td>9.716341e-06</td>\n",
       "      <td>1.142121e-05</td>\n",
       "      <td>1.452360e-06</td>\n",
       "      <td>7.451212e-06</td>\n",
       "      <td>3.891959e-01</td>\n",
       "      <td>6.507511e-04</td>\n",
       "      <td>...</td>\n",
       "      <td>2.628752e-03</td>\n",
       "      <td>8.146203e-06</td>\n",
       "      <td>9.905773e-05</td>\n",
       "      <td>4.940002e-03</td>\n",
       "      <td>1.494479e-05</td>\n",
       "      <td>1.489406e-04</td>\n",
       "      <td>6.847281e-04</td>\n",
       "      <td>3.307688e-08</td>\n",
       "      <td>5.601824e-07</td>\n",
       "      <td>2.500235e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>577</th>\n",
       "      <td>1535</td>\n",
       "      <td>9.144512e-07</td>\n",
       "      <td>1.963725e-05</td>\n",
       "      <td>5.338209e-06</td>\n",
       "      <td>5.140574e-06</td>\n",
       "      <td>4.465859e-06</td>\n",
       "      <td>4.210118e-05</td>\n",
       "      <td>2.216746e-04</td>\n",
       "      <td>4.652194e-08</td>\n",
       "      <td>3.508569e-07</td>\n",
       "      <td>...</td>\n",
       "      <td>7.436212e-08</td>\n",
       "      <td>2.402078e-05</td>\n",
       "      <td>2.000899e-08</td>\n",
       "      <td>5.199976e-07</td>\n",
       "      <td>1.354533e-06</td>\n",
       "      <td>1.383840e-08</td>\n",
       "      <td>3.563416e-09</td>\n",
       "      <td>1.587994e-04</td>\n",
       "      <td>7.331726e-06</td>\n",
       "      <td>1.344010e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>578</th>\n",
       "      <td>1537</td>\n",
       "      <td>9.687833e-05</td>\n",
       "      <td>2.085382e-06</td>\n",
       "      <td>7.982521e-08</td>\n",
       "      <td>5.022424e-04</td>\n",
       "      <td>5.473427e-07</td>\n",
       "      <td>2.792008e-08</td>\n",
       "      <td>1.326905e-05</td>\n",
       "      <td>2.426850e-07</td>\n",
       "      <td>1.008197e-04</td>\n",
       "      <td>...</td>\n",
       "      <td>1.538189e-05</td>\n",
       "      <td>2.140386e-06</td>\n",
       "      <td>4.838420e-05</td>\n",
       "      <td>1.327290e-06</td>\n",
       "      <td>9.910218e-01</td>\n",
       "      <td>5.429880e-03</td>\n",
       "      <td>8.591764e-05</td>\n",
       "      <td>2.960259e-06</td>\n",
       "      <td>4.227139e-06</td>\n",
       "      <td>2.757913e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>579</th>\n",
       "      <td>1540</td>\n",
       "      <td>9.895287e-01</td>\n",
       "      <td>1.542986e-05</td>\n",
       "      <td>5.060944e-05</td>\n",
       "      <td>7.646805e-05</td>\n",
       "      <td>5.800184e-06</td>\n",
       "      <td>1.490902e-05</td>\n",
       "      <td>1.599540e-06</td>\n",
       "      <td>9.094854e-07</td>\n",
       "      <td>5.415284e-04</td>\n",
       "      <td>...</td>\n",
       "      <td>5.096847e-06</td>\n",
       "      <td>2.255301e-05</td>\n",
       "      <td>5.876889e-06</td>\n",
       "      <td>7.283266e-06</td>\n",
       "      <td>6.827217e-05</td>\n",
       "      <td>2.174877e-07</td>\n",
       "      <td>1.424947e-06</td>\n",
       "      <td>2.184852e-07</td>\n",
       "      <td>5.558913e-06</td>\n",
       "      <td>1.164323e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>580</th>\n",
       "      <td>1542</td>\n",
       "      <td>2.078380e-03</td>\n",
       "      <td>1.599938e-05</td>\n",
       "      <td>1.554713e-06</td>\n",
       "      <td>1.189108e-03</td>\n",
       "      <td>2.295169e-05</td>\n",
       "      <td>9.253351e-07</td>\n",
       "      <td>1.297295e-05</td>\n",
       "      <td>1.617167e-06</td>\n",
       "      <td>3.125421e-05</td>\n",
       "      <td>...</td>\n",
       "      <td>1.158827e-05</td>\n",
       "      <td>1.584893e-05</td>\n",
       "      <td>8.685658e-05</td>\n",
       "      <td>3.978105e-05</td>\n",
       "      <td>1.162982e-02</td>\n",
       "      <td>1.526837e-05</td>\n",
       "      <td>3.593869e-06</td>\n",
       "      <td>6.624367e-06</td>\n",
       "      <td>1.078735e-05</td>\n",
       "      <td>5.277426e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>581</th>\n",
       "      <td>1546</td>\n",
       "      <td>6.892263e-05</td>\n",
       "      <td>3.726466e-06</td>\n",
       "      <td>2.881223e-05</td>\n",
       "      <td>4.457984e-06</td>\n",
       "      <td>3.178439e-06</td>\n",
       "      <td>5.270025e-06</td>\n",
       "      <td>9.272258e-06</td>\n",
       "      <td>4.025800e-07</td>\n",
       "      <td>1.353514e-05</td>\n",
       "      <td>...</td>\n",
       "      <td>5.997249e-08</td>\n",
       "      <td>1.142544e-06</td>\n",
       "      <td>2.173937e-06</td>\n",
       "      <td>2.275491e-06</td>\n",
       "      <td>9.393289e-07</td>\n",
       "      <td>1.432724e-07</td>\n",
       "      <td>8.683111e-08</td>\n",
       "      <td>1.828687e-05</td>\n",
       "      <td>1.757651e-06</td>\n",
       "      <td>1.107994e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>582</th>\n",
       "      <td>1553</td>\n",
       "      <td>7.844702e-05</td>\n",
       "      <td>1.223399e-05</td>\n",
       "      <td>4.042598e-04</td>\n",
       "      <td>8.170615e-04</td>\n",
       "      <td>2.019643e-06</td>\n",
       "      <td>5.760982e-06</td>\n",
       "      <td>6.445798e-05</td>\n",
       "      <td>4.818383e-07</td>\n",
       "      <td>7.553092e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>4.565838e-06</td>\n",
       "      <td>3.773053e-04</td>\n",
       "      <td>1.712005e-06</td>\n",
       "      <td>1.005428e-04</td>\n",
       "      <td>2.919866e-04</td>\n",
       "      <td>8.385166e-03</td>\n",
       "      <td>2.048941e-06</td>\n",
       "      <td>1.716510e-05</td>\n",
       "      <td>3.180879e-06</td>\n",
       "      <td>1.232045e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>583</th>\n",
       "      <td>1558</td>\n",
       "      <td>3.041244e-05</td>\n",
       "      <td>4.280318e-07</td>\n",
       "      <td>2.263313e-09</td>\n",
       "      <td>4.027959e-04</td>\n",
       "      <td>1.607721e-08</td>\n",
       "      <td>4.131885e-09</td>\n",
       "      <td>6.840258e-08</td>\n",
       "      <td>9.765561e-09</td>\n",
       "      <td>6.053516e-07</td>\n",
       "      <td>...</td>\n",
       "      <td>2.556076e-06</td>\n",
       "      <td>4.668253e-09</td>\n",
       "      <td>9.964290e-01</td>\n",
       "      <td>8.751773e-09</td>\n",
       "      <td>5.414948e-05</td>\n",
       "      <td>4.258681e-07</td>\n",
       "      <td>1.408075e-05</td>\n",
       "      <td>5.789859e-08</td>\n",
       "      <td>1.958021e-03</td>\n",
       "      <td>8.345197e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>584</th>\n",
       "      <td>1560</td>\n",
       "      <td>4.307395e-07</td>\n",
       "      <td>5.812215e-05</td>\n",
       "      <td>1.280462e-03</td>\n",
       "      <td>2.323053e-06</td>\n",
       "      <td>3.123312e-06</td>\n",
       "      <td>3.600927e-04</td>\n",
       "      <td>5.147393e-04</td>\n",
       "      <td>1.802469e-08</td>\n",
       "      <td>6.443024e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>5.475267e-08</td>\n",
       "      <td>3.684124e-05</td>\n",
       "      <td>1.052286e-07</td>\n",
       "      <td>4.832867e-07</td>\n",
       "      <td>6.025325e-07</td>\n",
       "      <td>6.531122e-09</td>\n",
       "      <td>1.810766e-08</td>\n",
       "      <td>8.124177e-05</td>\n",
       "      <td>4.408273e-05</td>\n",
       "      <td>1.815893e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>585</th>\n",
       "      <td>1564</td>\n",
       "      <td>6.653734e-10</td>\n",
       "      <td>7.175945e-09</td>\n",
       "      <td>6.942802e-08</td>\n",
       "      <td>1.428272e-05</td>\n",
       "      <td>6.045334e-09</td>\n",
       "      <td>1.604468e-09</td>\n",
       "      <td>2.191595e-07</td>\n",
       "      <td>2.790969e-09</td>\n",
       "      <td>5.953605e-10</td>\n",
       "      <td>...</td>\n",
       "      <td>1.437987e-08</td>\n",
       "      <td>5.414889e-06</td>\n",
       "      <td>3.596129e-10</td>\n",
       "      <td>1.114585e-04</td>\n",
       "      <td>1.544556e-07</td>\n",
       "      <td>3.633127e-08</td>\n",
       "      <td>1.164157e-09</td>\n",
       "      <td>3.847423e-05</td>\n",
       "      <td>9.047508e-08</td>\n",
       "      <td>9.830858e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>586</th>\n",
       "      <td>1565</td>\n",
       "      <td>2.645104e-06</td>\n",
       "      <td>2.698400e-05</td>\n",
       "      <td>2.979387e-06</td>\n",
       "      <td>7.124689e-04</td>\n",
       "      <td>3.275913e-06</td>\n",
       "      <td>1.168092e-05</td>\n",
       "      <td>1.083589e-05</td>\n",
       "      <td>6.816832e-07</td>\n",
       "      <td>5.750035e-07</td>\n",
       "      <td>...</td>\n",
       "      <td>2.211489e-07</td>\n",
       "      <td>9.803182e-06</td>\n",
       "      <td>2.219193e-06</td>\n",
       "      <td>2.029841e-05</td>\n",
       "      <td>1.268999e-05</td>\n",
       "      <td>7.063914e-05</td>\n",
       "      <td>2.394128e-06</td>\n",
       "      <td>1.598334e-03</td>\n",
       "      <td>2.671982e-06</td>\n",
       "      <td>1.091853e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>587</th>\n",
       "      <td>1567</td>\n",
       "      <td>3.037503e-05</td>\n",
       "      <td>4.191654e-09</td>\n",
       "      <td>6.884556e-09</td>\n",
       "      <td>7.751190e-05</td>\n",
       "      <td>6.845823e-09</td>\n",
       "      <td>2.158396e-08</td>\n",
       "      <td>1.540072e-08</td>\n",
       "      <td>7.139981e-11</td>\n",
       "      <td>3.745280e-10</td>\n",
       "      <td>...</td>\n",
       "      <td>7.119336e-08</td>\n",
       "      <td>2.059124e-06</td>\n",
       "      <td>1.470162e-04</td>\n",
       "      <td>3.603020e-08</td>\n",
       "      <td>3.891793e-05</td>\n",
       "      <td>2.783877e-10</td>\n",
       "      <td>7.203493e-07</td>\n",
       "      <td>2.503892e-07</td>\n",
       "      <td>4.555295e-03</td>\n",
       "      <td>4.030536e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>588</th>\n",
       "      <td>1573</td>\n",
       "      <td>6.762116e-10</td>\n",
       "      <td>1.206329e-08</td>\n",
       "      <td>6.531733e-07</td>\n",
       "      <td>5.843207e-08</td>\n",
       "      <td>4.676208e-09</td>\n",
       "      <td>3.793033e-08</td>\n",
       "      <td>1.569260e-06</td>\n",
       "      <td>3.490604e-11</td>\n",
       "      <td>2.179371e-10</td>\n",
       "      <td>...</td>\n",
       "      <td>1.224727e-06</td>\n",
       "      <td>6.215131e-07</td>\n",
       "      <td>1.307622e-06</td>\n",
       "      <td>5.177917e-09</td>\n",
       "      <td>6.200809e-07</td>\n",
       "      <td>2.687385e-09</td>\n",
       "      <td>3.592049e-06</td>\n",
       "      <td>2.591112e-05</td>\n",
       "      <td>1.875075e-04</td>\n",
       "      <td>8.417091e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>589</th>\n",
       "      <td>1576</td>\n",
       "      <td>3.725435e-05</td>\n",
       "      <td>9.835897e-01</td>\n",
       "      <td>3.741019e-05</td>\n",
       "      <td>2.292239e-05</td>\n",
       "      <td>3.831733e-03</td>\n",
       "      <td>7.615476e-06</td>\n",
       "      <td>5.792919e-06</td>\n",
       "      <td>1.256427e-05</td>\n",
       "      <td>2.596040e-03</td>\n",
       "      <td>...</td>\n",
       "      <td>1.178783e-06</td>\n",
       "      <td>8.460949e-06</td>\n",
       "      <td>4.190156e-06</td>\n",
       "      <td>1.524480e-06</td>\n",
       "      <td>4.240880e-07</td>\n",
       "      <td>3.866423e-06</td>\n",
       "      <td>2.993269e-05</td>\n",
       "      <td>1.691471e-07</td>\n",
       "      <td>2.301030e-07</td>\n",
       "      <td>2.162903e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>590</th>\n",
       "      <td>1577</td>\n",
       "      <td>1.402648e-05</td>\n",
       "      <td>6.748533e-05</td>\n",
       "      <td>5.680818e-07</td>\n",
       "      <td>2.488363e-04</td>\n",
       "      <td>2.475125e-06</td>\n",
       "      <td>2.656253e-07</td>\n",
       "      <td>3.235472e-05</td>\n",
       "      <td>2.097112e-05</td>\n",
       "      <td>1.912518e-03</td>\n",
       "      <td>...</td>\n",
       "      <td>3.898228e-06</td>\n",
       "      <td>3.918939e-06</td>\n",
       "      <td>2.652061e-04</td>\n",
       "      <td>2.660934e-07</td>\n",
       "      <td>1.081665e-02</td>\n",
       "      <td>1.526817e-02</td>\n",
       "      <td>4.437386e-04</td>\n",
       "      <td>1.267202e-06</td>\n",
       "      <td>3.512173e-06</td>\n",
       "      <td>1.939201e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>591</th>\n",
       "      <td>1579</td>\n",
       "      <td>4.779990e-05</td>\n",
       "      <td>5.940037e-06</td>\n",
       "      <td>2.041256e-05</td>\n",
       "      <td>5.014496e-07</td>\n",
       "      <td>1.052173e-05</td>\n",
       "      <td>3.185220e-05</td>\n",
       "      <td>3.290354e-06</td>\n",
       "      <td>2.285695e-07</td>\n",
       "      <td>2.727617e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>4.324647e-08</td>\n",
       "      <td>5.161604e-07</td>\n",
       "      <td>7.085660e-07</td>\n",
       "      <td>4.959128e-07</td>\n",
       "      <td>2.538120e-07</td>\n",
       "      <td>7.945127e-09</td>\n",
       "      <td>4.340505e-08</td>\n",
       "      <td>2.199025e-05</td>\n",
       "      <td>7.113237e-07</td>\n",
       "      <td>7.509444e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>592</th>\n",
       "      <td>1580</td>\n",
       "      <td>9.495337e-07</td>\n",
       "      <td>1.082657e-06</td>\n",
       "      <td>2.568015e-05</td>\n",
       "      <td>8.881887e-05</td>\n",
       "      <td>6.986738e-06</td>\n",
       "      <td>5.220055e-06</td>\n",
       "      <td>4.819686e-05</td>\n",
       "      <td>9.050057e-04</td>\n",
       "      <td>1.558449e-07</td>\n",
       "      <td>...</td>\n",
       "      <td>1.381393e-05</td>\n",
       "      <td>2.105317e-04</td>\n",
       "      <td>2.904952e-08</td>\n",
       "      <td>2.055476e-03</td>\n",
       "      <td>1.523315e-06</td>\n",
       "      <td>2.901100e-05</td>\n",
       "      <td>4.452886e-08</td>\n",
       "      <td>1.315977e-06</td>\n",
       "      <td>7.016382e-07</td>\n",
       "      <td>1.636394e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>593</th>\n",
       "      <td>1583</td>\n",
       "      <td>2.088253e-10</td>\n",
       "      <td>1.366735e-06</td>\n",
       "      <td>4.118992e-07</td>\n",
       "      <td>3.479119e-06</td>\n",
       "      <td>4.117404e-08</td>\n",
       "      <td>4.555689e-05</td>\n",
       "      <td>7.602586e-05</td>\n",
       "      <td>2.622256e-09</td>\n",
       "      <td>2.448347e-08</td>\n",
       "      <td>...</td>\n",
       "      <td>4.542656e-09</td>\n",
       "      <td>2.347915e-08</td>\n",
       "      <td>6.163290e-10</td>\n",
       "      <td>5.923407e-09</td>\n",
       "      <td>1.105520e-08</td>\n",
       "      <td>1.882911e-06</td>\n",
       "      <td>2.523917e-09</td>\n",
       "      <td>2.158796e-07</td>\n",
       "      <td>3.903763e-08</td>\n",
       "      <td>7.964114e-07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>594 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       id  Acer_Capillipes  Acer_Circinatum     Acer_Mono   Acer_Opalus  \\\n",
       "0       4     5.100685e-06     2.412410e-06  5.877471e-07  1.937902e-03   \n",
       "1       7     4.312378e-06     1.142324e-05  2.505422e-05  2.276995e-04   \n",
       "2       9     3.103504e-05     9.649146e-01  2.428891e-05  3.780332e-05   \n",
       "3      12     2.556006e-06     1.693535e-03  3.073582e-06  2.032537e-06   \n",
       "4      13     6.189646e-06     2.715071e-05  1.032662e-07  5.691051e-07   \n",
       "5      16     3.277398e-04     8.372749e-05  5.893797e-05  6.800114e-01   \n",
       "6      19     1.448475e-04     5.999024e-05  3.009324e-05  9.600083e-01   \n",
       "7      23     3.171916e-07     7.500980e-05  8.480211e-05  5.348376e-05   \n",
       "8      24     1.640448e-04     1.104876e-05  9.477199e-06  6.789436e-08   \n",
       "9      28     1.466255e-03     1.072608e-03  2.352958e-05  5.261607e-06   \n",
       "10     33     1.710005e-05     2.876335e-06  1.148133e-04  3.329956e-04   \n",
       "11     36     2.169724e-03     1.590941e-05  3.369599e-05  7.622061e-06   \n",
       "12     39     2.837284e-04     2.425320e-07  2.382951e-05  5.111489e-05   \n",
       "13     41     2.684766e-05     2.544118e-06  1.507194e-03  4.238139e-05   \n",
       "14     44     1.674098e-07     1.609267e-07  9.203021e-08  2.505952e-04   \n",
       "15     46     1.562715e-05     1.402860e-04  2.414944e-05  3.654345e-03   \n",
       "16     47     1.207388e-05     2.264785e-05  9.871576e-04  2.488431e-07   \n",
       "17     51     5.045758e-05     3.254994e-05  5.993143e-03  6.478239e-05   \n",
       "18     52     6.458659e-05     7.858984e-05  6.909396e-06  1.846784e-05   \n",
       "19     53     3.303559e-05     5.470498e-06  8.638493e-08  1.045149e-06   \n",
       "20     57     1.720937e-07     5.862953e-08  3.033212e-05  2.121668e-08   \n",
       "21     59     3.919239e-04     2.196559e-06  5.180858e-03  4.802334e-05   \n",
       "22     62     1.849914e-10     5.092108e-09  2.728949e-09  1.389164e-08   \n",
       "23     65     7.326046e-09     1.146629e-07  5.490738e-06  6.736457e-08   \n",
       "24     68     6.936047e-06     9.891200e-01  2.407083e-06  2.343225e-06   \n",
       "25     70     3.540725e-06     5.957349e-06  2.062797e-04  3.795326e-06   \n",
       "26     74     1.316960e-08     2.368121e-06  4.863270e-06  1.213677e-05   \n",
       "27     77     1.609329e-11     1.666645e-07  1.330036e-08  7.304642e-07   \n",
       "28     79     3.597878e-04     4.525925e-06  5.259288e-05  6.205271e-07   \n",
       "29     86     1.329394e-06     4.409827e-07  2.187596e-05  7.258432e-07   \n",
       "..    ...              ...              ...           ...           ...   \n",
       "564  1493     3.031251e-05     2.678755e-03  1.662585e-04  3.174586e-05   \n",
       "565  1495     1.318739e-06     5.894526e-07  3.799069e-05  2.246228e-06   \n",
       "566  1497     6.955544e-03     1.067015e-02  5.447153e-06  2.407807e-04   \n",
       "567  1498     8.132478e-08     1.132570e-06  1.779398e-05  1.368421e-05   \n",
       "568  1503     2.424580e-06     1.151860e-06  4.220916e-06  5.204691e-07   \n",
       "569  1510     2.044535e-05     4.491383e-05  9.465865e-08  1.733130e-07   \n",
       "570  1513     4.152271e-04     1.126968e-04  3.095721e-06  4.596747e-03   \n",
       "571  1517     2.458563e-05     4.847772e-06  7.619724e-06  5.863536e-07   \n",
       "572  1522     6.305919e-06     3.478516e-06  3.914444e-06  3.993074e-04   \n",
       "573  1526     5.165176e-05     1.390774e-06  4.313419e-05  7.544258e-04   \n",
       "574  1528     4.655133e-07     9.218487e-06  1.085533e-06  9.187168e-05   \n",
       "575  1533     5.545036e-06     1.473832e-05  2.347347e-06  1.054847e-03   \n",
       "576  1534     9.334250e-05     1.520183e-04  2.370013e-06  9.716341e-06   \n",
       "577  1535     9.144512e-07     1.963725e-05  5.338209e-06  5.140574e-06   \n",
       "578  1537     9.687833e-05     2.085382e-06  7.982521e-08  5.022424e-04   \n",
       "579  1540     9.895287e-01     1.542986e-05  5.060944e-05  7.646805e-05   \n",
       "580  1542     2.078380e-03     1.599938e-05  1.554713e-06  1.189108e-03   \n",
       "581  1546     6.892263e-05     3.726466e-06  2.881223e-05  4.457984e-06   \n",
       "582  1553     7.844702e-05     1.223399e-05  4.042598e-04  8.170615e-04   \n",
       "583  1558     3.041244e-05     4.280318e-07  2.263313e-09  4.027959e-04   \n",
       "584  1560     4.307395e-07     5.812215e-05  1.280462e-03  2.323053e-06   \n",
       "585  1564     6.653734e-10     7.175945e-09  6.942802e-08  1.428272e-05   \n",
       "586  1565     2.645104e-06     2.698400e-05  2.979387e-06  7.124689e-04   \n",
       "587  1567     3.037503e-05     4.191654e-09  6.884556e-09  7.751190e-05   \n",
       "588  1573     6.762116e-10     1.206329e-08  6.531733e-07  5.843207e-08   \n",
       "589  1576     3.725435e-05     9.835897e-01  3.741019e-05  2.292239e-05   \n",
       "590  1577     1.402648e-05     6.748533e-05  5.680818e-07  2.488363e-04   \n",
       "591  1579     4.779990e-05     5.940037e-06  2.041256e-05  5.014496e-07   \n",
       "592  1580     9.495337e-07     1.082657e-06  2.568015e-05  8.881887e-05   \n",
       "593  1583     2.088253e-10     1.366735e-06  4.118992e-07  3.479119e-06   \n",
       "\n",
       "     Acer_Palmatum   Acer_Pictum  Acer_Platanoids   Acer_Rubrum  \\\n",
       "0     9.898745e-07  1.415328e-07     1.922456e-05  1.263543e-06   \n",
       "1     1.421900e-05  1.741409e-05     3.877239e-04  1.598168e-05   \n",
       "2     6.383263e-03  6.635626e-05     8.623439e-06  5.011155e-04   \n",
       "3     1.603946e-05  8.469344e-07     2.134096e-04  4.118432e-05   \n",
       "4     7.512084e-06  7.799451e-08     3.097986e-05  2.140760e-06   \n",
       "5     5.904526e-05  1.395774e-05     2.266977e-04  5.485127e-03   \n",
       "6     1.400709e-05  5.621324e-06     5.499132e-05  3.559266e-05   \n",
       "7     1.010804e-05  1.988761e-03     3.183402e-05  2.463548e-07   \n",
       "8     1.030943e-05  1.476463e-04     1.856878e-04  3.971901e-06   \n",
       "9     3.463529e-05  1.082120e-05     7.083551e-05  3.812846e-04   \n",
       "10    1.802546e-05  7.363225e-05     4.020051e-05  2.779674e-06   \n",
       "11    9.362630e-05  9.973985e-06     5.153053e-06  2.001973e-05   \n",
       "12    6.774516e-07  8.899924e-07     1.507555e-07  1.619520e-06   \n",
       "13    1.368411e-06  1.033137e-05     2.483299e-05  1.146267e-05   \n",
       "14    1.071842e-06  2.432817e-07     9.574599e-07  2.764120e-06   \n",
       "15    6.076969e-06  7.852581e-06     2.972598e-04  2.402261e-06   \n",
       "16    9.085968e-06  7.195359e-05     1.651239e-03  2.449073e-06   \n",
       "17    3.498209e-05  2.599840e-04     7.917164e-05  2.176974e-03   \n",
       "18    2.509887e-05  6.017070e-06     1.256750e-04  1.909471e-02   \n",
       "19    7.204713e-06  1.689403e-04     3.395687e-05  6.433523e-07   \n",
       "20    7.182595e-07  1.384003e-06     8.721802e-09  1.091787e-08   \n",
       "21    2.717825e-05  1.060547e-05     2.782068e-06  3.496424e-06   \n",
       "22    1.050887e-08  2.051834e-08     6.172512e-08  2.240932e-08   \n",
       "23    1.160043e-07  9.411501e-08     5.197959e-04  8.424094e-09   \n",
       "24    9.198199e-05  2.759043e-06     8.491110e-06  4.936474e-05   \n",
       "25    1.154682e-05  1.220005e-04     4.646279e-06  6.024828e-07   \n",
       "26    2.530349e-06  1.810221e-04     9.473308e-06  4.328331e-07   \n",
       "27    2.370336e-08  1.429611e-06     9.244211e-06  4.426106e-09   \n",
       "28    4.145176e-05  4.521232e-04     9.294804e-07  2.173182e-07   \n",
       "29    2.218561e-06  3.322652e-05     2.369600e-06  8.624168e-08   \n",
       "..             ...           ...              ...           ...   \n",
       "564   3.930589e-04  9.402625e-04     6.509766e-04  5.307657e-05   \n",
       "565   2.753388e-06  1.666475e-06     4.043600e-06  2.698249e-04   \n",
       "566   9.688936e-04  1.117147e-05     2.433453e-05  3.297738e-04   \n",
       "567   8.691900e-07  7.981706e-07     1.261864e-05  8.118120e-06   \n",
       "568   9.024905e-07  4.478792e-06     5.061892e-06  7.543643e-08   \n",
       "569   1.498447e-05  1.583407e-07     1.270387e-05  4.367338e-06   \n",
       "570   3.315627e-05  4.419860e-07     6.062882e-05  3.664158e-05   \n",
       "571   4.027911e-06  3.037552e-04     4.051010e-03  7.858964e-07   \n",
       "572   2.193975e-06  1.184731e-06     6.706700e-07  2.535064e-05   \n",
       "573   1.750050e-06  6.887137e-06     1.366514e-04  1.356353e-06   \n",
       "574   9.398969e-06  1.608738e-06     3.769889e-05  4.232019e-07   \n",
       "575   6.276145e-06  2.731647e-06     1.743846e-05  2.094332e-05   \n",
       "576   1.142121e-05  1.452360e-06     7.451212e-06  3.891959e-01   \n",
       "577   4.465859e-06  4.210118e-05     2.216746e-04  4.652194e-08   \n",
       "578   5.473427e-07  2.792008e-08     1.326905e-05  2.426850e-07   \n",
       "579   5.800184e-06  1.490902e-05     1.599540e-06  9.094854e-07   \n",
       "580   2.295169e-05  9.253351e-07     1.297295e-05  1.617167e-06   \n",
       "581   3.178439e-06  5.270025e-06     9.272258e-06  4.025800e-07   \n",
       "582   2.019643e-06  5.760982e-06     6.445798e-05  4.818383e-07   \n",
       "583   1.607721e-08  4.131885e-09     6.840258e-08  9.765561e-09   \n",
       "584   3.123312e-06  3.600927e-04     5.147393e-04  1.802469e-08   \n",
       "585   6.045334e-09  1.604468e-09     2.191595e-07  2.790969e-09   \n",
       "586   3.275913e-06  1.168092e-05     1.083589e-05  6.816832e-07   \n",
       "587   6.845823e-09  2.158396e-08     1.540072e-08  7.139981e-11   \n",
       "588   4.676208e-09  3.793033e-08     1.569260e-06  3.490604e-11   \n",
       "589   3.831733e-03  7.615476e-06     5.792919e-06  1.256427e-05   \n",
       "590   2.475125e-06  2.656253e-07     3.235472e-05  2.097112e-05   \n",
       "591   1.052173e-05  3.185220e-05     3.290354e-06  2.285695e-07   \n",
       "592   6.986738e-06  5.220055e-06     4.819686e-05  9.050057e-04   \n",
       "593   4.117404e-08  4.555689e-05     7.602586e-05  2.622256e-09   \n",
       "\n",
       "     Acer_Rufinerve       ...         Salix_Fragilis  Salix_Intergra  \\\n",
       "0      5.693773e-07       ...           1.015428e-06    4.848693e-04   \n",
       "1      2.164532e-06       ...           3.267626e-06    2.513821e-05   \n",
       "2      3.080307e-03       ...           7.580579e-05    1.971487e-05   \n",
       "3      1.491737e-03       ...           1.048205e-04    2.698628e-06   \n",
       "4      1.722694e-04       ...           7.931955e-05    2.736775e-07   \n",
       "5      1.992310e-04       ...           1.304590e-03    6.563566e-04   \n",
       "6      1.087574e-05       ...           1.444610e-05    2.158408e-04   \n",
       "7      3.895387e-08       ...           9.946294e-07    4.765113e-05   \n",
       "8      3.189352e-04       ...           2.750560e-06    1.884413e-06   \n",
       "9      9.893776e-01       ...           2.031515e-04    1.541470e-05   \n",
       "10     9.358470e-08       ...           4.902801e-05    2.559539e-05   \n",
       "11     1.381119e-04       ...           2.137712e-06    3.481058e-06   \n",
       "12     2.468836e-07       ...           6.718647e-06    3.426693e-04   \n",
       "13     2.548856e-06       ...           2.957194e-05    3.907455e-05   \n",
       "14     3.545994e-06       ...           5.824529e-06    1.348715e-06   \n",
       "15     2.987228e-06       ...           7.660515e-06    7.035894e-06   \n",
       "16     4.065607e-04       ...           7.360163e-07    2.649848e-05   \n",
       "17     1.964623e-06       ...           6.204216e-05    6.928353e-04   \n",
       "18     2.828155e-04       ...           1.061772e-03    3.933068e-06   \n",
       "19     1.694212e-07       ...           5.804612e-06    1.711523e-05   \n",
       "20     1.002931e-09       ...           2.828729e-07    4.796914e-04   \n",
       "21     3.371314e-06       ...           2.769726e-06    7.730975e-04   \n",
       "22     7.793210e-11       ...           2.078816e-06    1.779209e-09   \n",
       "23     2.776460e-06       ...           1.101705e-06    9.585277e-07   \n",
       "24     1.598945e-04       ...           3.235514e-05    1.792711e-05   \n",
       "25     7.160270e-07       ...           1.119259e-06    3.767865e-05   \n",
       "26     3.592488e-08       ...           9.658292e-06    2.263426e-06   \n",
       "27     3.221968e-09       ...           1.763902e-08    6.098416e-09   \n",
       "28     1.206652e-05       ...           1.881741e-06    1.801210e-05   \n",
       "29     4.286561e-07       ...           1.410489e-07    1.616385e-06   \n",
       "..              ...       ...                    ...             ...   \n",
       "564    1.173775e-05       ...           1.122894e-03    8.123437e-05   \n",
       "565    1.813196e-07       ...           4.632004e-05    2.314743e-05   \n",
       "566    1.704964e-02       ...           1.852340e-03    2.580321e-05   \n",
       "567    8.602336e-07       ...           3.492281e-05    1.924845e-05   \n",
       "568    4.444979e-07       ...           3.333730e-07    6.571474e-06   \n",
       "569    3.768804e-04       ...           1.299653e-04    1.908526e-07   \n",
       "570    4.884847e-04       ...           1.385203e-04    5.319734e-05   \n",
       "571    1.244709e-05       ...           4.038423e-06    7.368244e-06   \n",
       "572    1.790579e-05       ...           7.179528e-07    1.392134e-05   \n",
       "573    1.719631e-06       ...           9.005345e-06    2.722215e-05   \n",
       "574    9.283282e-07       ...           1.302681e-07    1.180830e-06   \n",
       "575    2.090858e-06       ...           8.631513e-07    4.310547e-05   \n",
       "576    6.507511e-04       ...           2.628752e-03    8.146203e-06   \n",
       "577    3.508569e-07       ...           7.436212e-08    2.402078e-05   \n",
       "578    1.008197e-04       ...           1.538189e-05    2.140386e-06   \n",
       "579    5.415284e-04       ...           5.096847e-06    2.255301e-05   \n",
       "580    3.125421e-05       ...           1.158827e-05    1.584893e-05   \n",
       "581    1.353514e-05       ...           5.997249e-08    1.142544e-06   \n",
       "582    7.553092e-06       ...           4.565838e-06    3.773053e-04   \n",
       "583    6.053516e-07       ...           2.556076e-06    4.668253e-09   \n",
       "584    6.443024e-06       ...           5.475267e-08    3.684124e-05   \n",
       "585    5.953605e-10       ...           1.437987e-08    5.414889e-06   \n",
       "586    5.750035e-07       ...           2.211489e-07    9.803182e-06   \n",
       "587    3.745280e-10       ...           7.119336e-08    2.059124e-06   \n",
       "588    2.179371e-10       ...           1.224727e-06    6.215131e-07   \n",
       "589    2.596040e-03       ...           1.178783e-06    8.460949e-06   \n",
       "590    1.912518e-03       ...           3.898228e-06    3.918939e-06   \n",
       "591    2.727617e-06       ...           4.324647e-08    5.161604e-07   \n",
       "592    1.558449e-07       ...           1.381393e-05    2.105317e-04   \n",
       "593    2.448347e-08       ...           4.542656e-09    2.347915e-08   \n",
       "\n",
       "      Sorbus_Aria  Tilia_Oliveri  Tilia_Platyphyllos  Tilia_Tomentosa  \\\n",
       "0    8.610005e-06   2.287367e-06        6.877683e-05     1.113913e-06   \n",
       "1    9.021912e-06   1.609234e-05        5.398817e-06     1.122376e-04   \n",
       "2    2.262111e-05   5.601712e-06        8.572335e-07     1.320894e-05   \n",
       "3    3.979252e-04   8.748960e-06        1.894345e-04     7.730371e-04   \n",
       "4    2.613650e-04   7.346861e-06        9.931298e-04     1.977421e-04   \n",
       "5    1.259644e-03   1.743636e-03        3.731517e-03     2.040616e-03   \n",
       "6    1.989583e-04   1.188127e-04        4.468976e-04     7.788631e-04   \n",
       "7    1.489128e-06   6.503938e-07        1.352986e-05     1.678264e-07   \n",
       "8    6.784715e-07   2.140089e-05        2.194002e-06     3.977391e-07   \n",
       "9    1.895658e-04   1.781775e-05        1.126358e-04     1.031415e-03   \n",
       "10   7.996816e-05   1.180693e-03        1.629108e-05     6.567297e-08   \n",
       "11   4.724215e-05   8.799047e-06        5.689618e-05     1.586029e-06   \n",
       "12   2.673141e-07   2.583342e-04        1.809465e-06     5.001092e-07   \n",
       "13   1.803960e-06   4.938464e-04        1.045530e-05     7.566721e-03   \n",
       "14   5.016460e-07   2.945733e-05        6.780209e-06     4.380234e-06   \n",
       "15   7.596071e-03   3.674875e-04        3.443590e-04     1.203780e-04   \n",
       "16   5.025420e-06   2.132082e-05        2.053418e-06     1.947693e-07   \n",
       "17   2.390751e-06   2.470055e-04        2.457492e-06     7.799804e-05   \n",
       "18   1.660814e-04   9.800599e-03        9.397652e-05     1.210757e-03   \n",
       "19   4.206014e-07   1.829545e-05        6.195688e-06     1.919363e-07   \n",
       "20   6.366831e-09   2.249857e-08        1.047165e-06     1.084100e-09   \n",
       "21   4.363312e-06   9.274812e-05        6.067779e-05     8.523924e-06   \n",
       "22   4.567689e-09   1.976186e-08        4.489929e-10     8.343788e-10   \n",
       "23   8.679514e-06   2.564097e-07        9.328999e-05     6.294757e-07   \n",
       "24   1.247719e-05   1.321841e-06        2.295438e-06     3.918069e-05   \n",
       "25   6.733817e-06   1.992526e-06        1.487477e-05     1.354031e-07   \n",
       "26   4.335928e-07   2.112389e-05        6.346430e-07     3.475899e-07   \n",
       "27   6.239500e-09   6.639810e-09        3.778765e-09     5.516613e-08   \n",
       "28   4.391960e-06   4.830018e-06        7.892981e-07     1.913674e-08   \n",
       "29   9.609012e-07   3.439576e-07        1.101925e-06     2.532985e-08   \n",
       "..            ...            ...                 ...              ...   \n",
       "564  4.250840e-03   2.020107e-03        1.089542e-04     1.398945e-05   \n",
       "565  1.319309e-07   9.819421e-01        2.434656e-06     2.250847e-06   \n",
       "566  1.486365e-03   8.562071e-05        1.202832e-02     2.199880e-03   \n",
       "567  1.341229e-06   3.806033e-04        3.704371e-06     7.969818e-07   \n",
       "568  2.538010e-08   1.871423e-07        8.040818e-08     7.105646e-10   \n",
       "569  5.652427e-04   7.709732e-06        1.114253e-04     6.898544e-05   \n",
       "570  8.648143e-04   4.635487e-05        9.628000e-01     6.207589e-03   \n",
       "571  1.003339e-05   1.812392e-05        1.039223e-04     1.330943e-06   \n",
       "572  1.020551e-05   1.000089e-06        2.098831e-06     2.484997e-04   \n",
       "573  6.478511e-06   2.531584e-04        3.706667e-05     2.506213e-05   \n",
       "574  4.263813e-07   5.122200e-07        2.693186e-06     8.946493e-08   \n",
       "575  9.184498e-06   8.554584e-06        1.984236e-06     1.776964e-04   \n",
       "576  9.905773e-05   4.940002e-03        1.494479e-05     1.489406e-04   \n",
       "577  2.000899e-08   5.199976e-07        1.354533e-06     1.383840e-08   \n",
       "578  4.838420e-05   1.327290e-06        9.910218e-01     5.429880e-03   \n",
       "579  5.876889e-06   7.283266e-06        6.827217e-05     2.174877e-07   \n",
       "580  8.685658e-05   3.978105e-05        1.162982e-02     1.526837e-05   \n",
       "581  2.173937e-06   2.275491e-06        9.393289e-07     1.432724e-07   \n",
       "582  1.712005e-06   1.005428e-04        2.919866e-04     8.385166e-03   \n",
       "583  9.964290e-01   8.751773e-09        5.414948e-05     4.258681e-07   \n",
       "584  1.052286e-07   4.832867e-07        6.025325e-07     6.531122e-09   \n",
       "585  3.596129e-10   1.114585e-04        1.544556e-07     3.633127e-08   \n",
       "586  2.219193e-06   2.029841e-05        1.268999e-05     7.063914e-05   \n",
       "587  1.470162e-04   3.603020e-08        3.891793e-05     2.783877e-10   \n",
       "588  1.307622e-06   5.177917e-09        6.200809e-07     2.687385e-09   \n",
       "589  4.190156e-06   1.524480e-06        4.240880e-07     3.866423e-06   \n",
       "590  2.652061e-04   2.660934e-07        1.081665e-02     1.526817e-02   \n",
       "591  7.085660e-07   4.959128e-07        2.538120e-07     7.945127e-09   \n",
       "592  2.904952e-08   2.055476e-03        1.523315e-06     2.901100e-05   \n",
       "593  6.163290e-10   5.923407e-09        1.105520e-08     1.882911e-06   \n",
       "\n",
       "     Ulmus_Bergmanniana  Viburnum_Tinus  Viburnum_x_Rhytidophylloides  \\\n",
       "0          1.810364e-07    2.309414e-06                  1.934680e-06   \n",
       "1          1.609923e-06    2.098621e-04                  3.769313e-06   \n",
       "2          5.410352e-05    7.606266e-07                  9.855883e-07   \n",
       "3          4.923149e-02    7.229227e-06                  2.527448e-05   \n",
       "4          5.775944e-04    1.175690e-05                  7.660232e-06   \n",
       "5          2.223019e-04    3.221096e-04                  5.891598e-03   \n",
       "6          1.071679e-05    1.625313e-04                  2.955268e-04   \n",
       "7          1.720844e-07    2.727328e-05                  2.883907e-05   \n",
       "8          1.224768e-07    2.685703e-08                  1.279548e-07   \n",
       "9          2.078585e-04    7.360682e-08                  1.073095e-06   \n",
       "10         1.172341e-04    7.284094e-03                  1.500942e-02   \n",
       "11         6.356699e-06    3.218537e-05                  3.966579e-06   \n",
       "12         6.828984e-07    4.173997e-05                  3.865789e-06   \n",
       "13         3.108142e-06    4.276371e-07                  6.408330e-06   \n",
       "14         1.526923e-07    6.336413e-04                  1.217707e-07   \n",
       "15         2.609295e-03    4.156264e-04                  2.332870e-04   \n",
       "16         1.219362e-07    9.635706e-08                  3.977082e-07   \n",
       "17         3.589343e-06    3.409938e-06                  8.829691e-05   \n",
       "18         8.812274e-04    7.285024e-06                  8.167784e-06   \n",
       "19         7.788410e-07    3.991335e-08                  2.020168e-07   \n",
       "20         1.520351e-08    1.043932e-03                  1.633731e-06   \n",
       "21         1.615564e-06    1.487565e-03                  2.856674e-04   \n",
       "22         2.393548e-09    1.763315e-07                  1.067737e-08   \n",
       "23         2.299215e-04    5.658289e-04                  2.393929e-03   \n",
       "24         1.490536e-04    2.371046e-07                  7.541565e-07   \n",
       "25         4.032670e-06    6.040760e-04                  8.687493e-05   \n",
       "26         1.326052e-06    2.264514e-04                  4.529306e-06   \n",
       "27         3.719345e-08    5.281830e-08                  5.918338e-08   \n",
       "28         4.143508e-07    4.020494e-09                  2.639540e-05   \n",
       "29         2.955292e-07    1.051313e-04                  3.253613e-05   \n",
       "..                  ...             ...                           ...   \n",
       "564        1.063141e-02    1.721183e-03                  4.969870e-04   \n",
       "565        7.040683e-07    5.364548e-06                  1.515567e-06   \n",
       "566        6.704818e-03    1.065958e-05                  2.095960e-05   \n",
       "567        1.285498e-05    1.977723e-04                  9.091429e-06   \n",
       "568        2.904834e-09    4.358912e-06                  1.129633e-05   \n",
       "569        6.618744e-04    1.248170e-05                  1.289161e-05   \n",
       "570        4.462679e-04    3.142048e-05                  9.819636e-05   \n",
       "571        2.411209e-06    4.176620e-07                  3.782613e-06   \n",
       "572        5.190415e-07    3.580616e-05                  5.964840e-07   \n",
       "573        1.150804e-06    3.022444e-05                  3.303706e-06   \n",
       "574        1.218531e-07    1.047218e-03                  2.453055e-05   \n",
       "575        1.524096e-06    7.978188e-05                  9.461069e-07   \n",
       "576        6.847281e-04    3.307688e-08                  5.601824e-07   \n",
       "577        3.563416e-09    1.587994e-04                  7.331726e-06   \n",
       "578        8.591764e-05    2.960259e-06                  4.227139e-06   \n",
       "579        1.424947e-06    2.184852e-07                  5.558913e-06   \n",
       "580        3.593869e-06    6.624367e-06                  1.078735e-05   \n",
       "581        8.683111e-08    1.828687e-05                  1.757651e-06   \n",
       "582        2.048941e-06    1.716510e-05                  3.180879e-06   \n",
       "583        1.408075e-05    5.789859e-08                  1.958021e-03   \n",
       "584        1.810766e-08    8.124177e-05                  4.408273e-05   \n",
       "585        1.164157e-09    3.847423e-05                  9.047508e-08   \n",
       "586        2.394128e-06    1.598334e-03                  2.671982e-06   \n",
       "587        7.203493e-07    2.503892e-07                  4.555295e-03   \n",
       "588        3.592049e-06    2.591112e-05                  1.875075e-04   \n",
       "589        2.993269e-05    1.691471e-07                  2.301030e-07   \n",
       "590        4.437386e-04    1.267202e-06                  3.512173e-06   \n",
       "591        4.340505e-08    2.199025e-05                  7.113237e-07   \n",
       "592        4.452886e-08    1.315977e-06                  7.016382e-07   \n",
       "593        2.523917e-09    2.158796e-07                  3.903763e-08   \n",
       "\n",
       "     Zelkova_Serrata  \n",
       "0       6.987196e-07  \n",
       "1       2.246706e-04  \n",
       "2       7.037049e-03  \n",
       "3       2.818535e-03  \n",
       "4       5.087229e-07  \n",
       "5       6.087327e-04  \n",
       "6       4.634736e-03  \n",
       "7       1.743627e-05  \n",
       "8       2.141610e-07  \n",
       "9       1.580574e-04  \n",
       "10      6.116187e-05  \n",
       "11      1.848037e-05  \n",
       "12      8.141697e-06  \n",
       "13      5.493495e-05  \n",
       "14      2.608506e-05  \n",
       "15      3.216509e-04  \n",
       "16      1.263164e-07  \n",
       "17      1.170716e-04  \n",
       "18      1.133546e-04  \n",
       "19      3.430661e-08  \n",
       "20      1.040544e-07  \n",
       "21      1.136834e-03  \n",
       "22      4.571497e-09  \n",
       "23      2.010694e-07  \n",
       "24      3.309535e-03  \n",
       "25      1.304432e-06  \n",
       "26      3.558214e-06  \n",
       "27      1.403673e-09  \n",
       "28      3.201887e-08  \n",
       "29      8.386794e-08  \n",
       "..               ...  \n",
       "564     7.579166e-05  \n",
       "565     2.494497e-06  \n",
       "566     2.428982e-04  \n",
       "567     1.724240e-05  \n",
       "568     5.789196e-07  \n",
       "569     4.290463e-07  \n",
       "570     7.166729e-05  \n",
       "571     1.748885e-08  \n",
       "572     6.358935e-04  \n",
       "573     1.862428e-06  \n",
       "574     1.344052e-04  \n",
       "575     7.704433e-04  \n",
       "576     2.500235e-05  \n",
       "577     1.344010e-06  \n",
       "578     2.757913e-07  \n",
       "579     1.164323e-05  \n",
       "580     5.277426e-07  \n",
       "581     1.107994e-05  \n",
       "582     1.232045e-04  \n",
       "583     8.345197e-07  \n",
       "584     1.815893e-05  \n",
       "585     9.830858e-07  \n",
       "586     1.091853e-03  \n",
       "587     4.030536e-09  \n",
       "588     8.417091e-08  \n",
       "589     2.162903e-03  \n",
       "590     1.939201e-03  \n",
       "591     7.509444e-07  \n",
       "592     1.636394e-06  \n",
       "593     7.964114e-07  \n",
       "\n",
       "[594 rows x 100 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_df = pd.DataFrame(preds_test, columns=data.le.classes_)\n",
    "ids_test_df = pd.DataFrame(ids_test, columns=[\"id\"])\n",
    "submission = pd.concat([ids_test_df, preds_df], axis=1)\n",
    "submission.to_csv('submission_mlp.csv', index=False)\n",
    "# below prints the submission, can be removed and replaced with code block below\n",
    "submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "\n",
    "When doing these exercises nothing is sacred, you can change learning rate, try testing various learning rates, batch sizes, validation sizes, etc. And most importantly, the validation set is very small (only 1 sample per class), so try different seeds if evaluating the same model twice.\n",
    "\n",
    "Describe how each of below tasks effects training:\n",
    "\n",
    "1. Set DROPOUT to TRUE\n",
    "2. Include L2 regularization\n",
    "3. Try with L1 regularization\n",
    "4. Use only the image for training (with CNN)\n",
    "5. Comment in dropout from CNN layers\n",
    "6. Include the RNN part - how does this affect training"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
